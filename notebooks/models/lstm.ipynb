{
 "cells": [
  {
   "cell_type": "code",
   "id": "56dabf357906194a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T02:12:47.540167228Z",
     "start_time": "2025-11-19T02:12:47.509840509Z"
    }
   },
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from spectrum.config import WINDOW_SIZE\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from spectrum.utils import set_random_state\n",
    "from spectrum.models import LSTM\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.edgecolor\": \"0.3\",\n",
    "        \"axes.linewidth\": 0.8,\n",
    "        \"font.size\": 12,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"figure.dpi\": 120,\n",
    "        \"legend.frameon\": False,\n",
    "    }\n",
    ")\n",
    "set_random_state()"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "92fcea11662cadf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T02:12:58.825995325Z",
     "start_time": "2025-11-19T02:12:47.541369213Z"
    }
   },
   "source": [
    "selected_ids = [4, 17, 33]\n",
    "\n",
    "results_dir = \"../../results/models/lstm\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def find_best_threshold(scores, true_labels, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        # use percentiles as candidate thresholds\n",
    "        thresholds = [np.percentile(scores, p) for p in range(50, 100, 1)]\n",
    "        # add some extra threshold points\n",
    "        thresholds.extend([np.percentile(scores, p) for p in [99.5, 99.9]])\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_threshold = thresholds[0]\n",
    "    best_metrics = {}\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (scores > threshold).astype(int)\n",
    "\n",
    "        # calculate confusion matrix\n",
    "        TP = ((true_labels == 1) & (pred_labels == 1)).sum()\n",
    "        FP = ((true_labels == 0) & (pred_labels == 1)).sum()\n",
    "        TN = ((true_labels == 0) & (pred_labels == 0)).sum()\n",
    "        FN = ((true_labels == 1) & (pred_labels == 0)).sum()\n",
    "\n",
    "        # calculate metrics\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                'threshold': threshold,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'fnr': fnr,\n",
    "                'fpr': fpr,\n",
    "                'tp': TP,\n",
    "                'fp': FP,\n",
    "                'tn': TN,\n",
    "                'fn': FN\n",
    "            }\n",
    "\n",
    "    return best_threshold, best_metrics\n",
    "\n",
    "\n",
    "def process_single_id(data_id):\n",
    "    print(f\"processing: {data_id}\")\n",
    "\n",
    "    train_data = pl.read_csv(f\"../../datasets/Yahoo/train/A1/{data_id}.csv\")\n",
    "    test_data = pl.read_csv(f\"../../datasets/Yahoo/test/A1/{data_id}.csv\")\n",
    "    complete_data = pl.read_csv(f\"../../datasets/Yahoo/data/A1Benchmark/{data_id}.csv\")\n",
    "\n",
    "    print(\"  training model...\")\n",
    "    model = LSTM()\n",
    "    start_time = time.time()\n",
    "    model.fit(train_data[\"value\"])\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    print(\"  anomaly detection...\")\n",
    "    start_time = time.time()\n",
    "    scores = model.predict(test_data[\"value\"])\n",
    "    scoring_time = time.time() - start_time\n",
    "\n",
    "    train_len = len(train_data)\n",
    "    scores_array = scores\n",
    "\n",
    "    test_true_labels = complete_data[\"label\"][train_len + WINDOW_SIZE:].to_numpy()\n",
    "\n",
    "    print(\"  finding best threshold...\")\n",
    "    best_threshold, best_metrics = find_best_threshold(scores_array, test_true_labels)\n",
    "\n",
    "    complete_values = complete_data[\"value\"].to_numpy()\n",
    "    complete_labels = complete_data[\"label\"].to_numpy()\n",
    "    complete_timestamps = complete_data[\"timestamp\"] if \"timestamp\" in complete_data.columns else range(\n",
    "        len(complete_values))\n",
    "\n",
    "    complete_predictions = np.zeros(len(complete_values))\n",
    "    complete_anomaly_scores = np.zeros(len(complete_values))\n",
    "\n",
    "    complete_predictions[train_len + WINDOW_SIZE:] = (scores_array > best_threshold).astype(int)\n",
    "    complete_anomaly_scores[train_len + WINDOW_SIZE:] = scores_array\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'timestamp': complete_timestamps,\n",
    "        'value': complete_values,\n",
    "        'label': complete_labels,\n",
    "        'predicted': complete_predictions,\n",
    "        'anomaly_score': complete_anomaly_scores\n",
    "    })\n",
    "\n",
    "    output_file = os.path.join(results_dir, f\"{data_id}.csv\")\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"  results saved to: {output_file}\")\n",
    "\n",
    "    return {\n",
    "        'id': data_id,\n",
    "        'training_time': training_time,\n",
    "        'testing_time': scoring_time,\n",
    "        'total_time': training_time + scoring_time,\n",
    "        'train_samples': len(train_data),\n",
    "        'test_samples': min_len,\n",
    "        'best_threshold': best_threshold,\n",
    "        **best_metrics\n",
    "    }\n",
    "\n",
    "\n",
    "all_results = []\n",
    "print(f\"processing {len(selected_ids)} datasets...\")\n",
    "\n",
    "for data_id in tqdm(selected_ids, desc=\"processing\"):\n",
    "    try:\n",
    "        result = process_single_id(data_id)\n",
    "        all_results.append(result)\n",
    "\n",
    "        print(f\"  ID {data_id} completed:\")\n",
    "        print(f\"    best_threshold: {result['best_threshold']:.4f}\")\n",
    "        print(f\"    f1: {result['f1']:.4f}\")\n",
    "        print(f\"    precision: {result['precision']:.4f}\")\n",
    "        print(f\"    recall: {result['recall']:.4f}\")\n",
    "        print(f\"    accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  processing {data_id} failed: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "if all_results:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    summary_file = os.path.join(results_dir, \"lstm.csv\")\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"summary results saved to: {summary_file}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LSTM anomaly detection results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"processed {len(all_results)} datasets\")\n",
    "    print(f\"average F1: {summary_df['f1'].mean():.4f} ± {summary_df['f1'].std():.4f}\")\n",
    "    print(f\"average precision: {summary_df['precision'].mean():.4f} ± {summary_df['precision'].std():.4f}\")\n",
    "    print(f\"average recall: {summary_df['recall'].mean():.4f} ± {summary_df['recall'].std():.4f}\")\n",
    "    print(f\"average accuracy: {summary_df['accuracy'].mean():.4f} ± {summary_df['accuracy'].std():.4f}\")\n",
    "    print(f\"average training time: {summary_df['training_time'].mean():.2f}s\")\n",
    "    print(f\"average scoring time: {summary_df['testing_time'].mean():.2f}s\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"details:\")\n",
    "    display_cols = ['id', 'f1', 'precision', 'recall', 'accuracy', 'best_threshold', 'tp', 'fp', 'tn', 'fn']\n",
    "    print(summary_df[display_cols].round(4))\n",
    "else:\n",
    "    print(\"no results\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 3 datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 4\n",
      "  training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:  33%|███▎      | 1/3 [00:06<00:12,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anomaly detection...\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/lstm/4.csv\n",
      "  ID 4 completed:\n",
      "    best_threshold: 190.0937\n",
      "    f1: 0.7826\n",
      "    precision: 0.6429\n",
      "    recall: 1.0000\n",
      "    accuracy: 0.9926\n",
      "processing: 17\n",
      "  training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:  67%|██████▋   | 2/3 [00:07<00:03,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anomaly detection...\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/lstm/17.csv\n",
      "  ID 17 completed:\n",
      "    best_threshold: 91.3125\n",
      "    f1: 0.9896\n",
      "    precision: 1.0000\n",
      "    recall: 0.9795\n",
      "    accuracy: 0.9956\n",
      "processing: 33\n",
      "  training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anomaly detection...\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/lstm/33.csv\n",
      "  ID 33 completed:\n",
      "    best_threshold: 4842.6587\n",
      "    f1: 1.0000\n",
      "    precision: 1.0000\n",
      "    recall: 1.0000\n",
      "    accuracy: 1.0000\n",
      "summary results saved to: ../../results/models/lstm/lstm.csv\n",
      "\n",
      "================================================================================\n",
      "LSTM anomaly detection results\n",
      "================================================================================\n",
      "processed 3 datasets\n",
      "average F1: 0.9241 ± 0.1226\n",
      "average precision: 0.8810 ± 0.2062\n",
      "average recall: 0.9932 ± 0.0119\n",
      "average accuracy: 0.9961 ± 0.0037\n",
      "average training time: 3.69s\n",
      "average scoring time: 0.04s\n",
      "================================================================================\n",
      "details:\n",
      "   id      f1  precision  recall  accuracy  best_threshold   tp  fp   tn  fn\n",
      "0   4  0.7826     0.6429  1.0000    0.9926      190.093704    9   5  666   0\n",
      "1  17  0.9896     1.0000  0.9795    0.9956       91.312500  143   0  534   3\n",
      "2  33  1.0000     1.0000  1.0000    1.0000     4842.658691    1   0  687   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
