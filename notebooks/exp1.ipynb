{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etniX_KTlJ5U"
   },
   "source": [
    "# USAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6u1DGKsAlLF-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "from spectrum.models import USAD\n",
    "from spectrum.models.sr.sr import SpectralResidual\n",
    "from spectrum.utils import device\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 100\n",
    "HIDDEN_SIZE = 100\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "results_dir = \"../../results/models/hybrid_sr_usad\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "selected_ids = [1, 2, 3]\n",
    "\n",
    "def find_best_threshold(scores, true_labels, thresholds=None):\n",
    "    if np.isnan(scores).any() or np.isinf(scores).any():\n",
    "        scores = np.nan_to_num(scores, nan=0.0, posinf=np.max(scores[np.isfinite(scores)]) if np.any(np.isfinite(scores)) else 1.0)\n",
    "\n",
    "    if thresholds is None:\n",
    "        if np.min(scores) == np.max(scores):\n",
    "            thresholds = [scores[0]]\n",
    "        else:\n",
    "            thresholds = [np.percentile(scores, p) for p in range(0, 90, 5)]\n",
    "            thresholds.extend([np.percentile(scores, p) for p in range(90, 100, 1)])\n",
    "            thresholds.extend([np.percentile(scores, p) for p in [99.1, 99.3, 99.5, 99.7, 99.9, 99.95, 99.99]])\n",
    "            \n",
    "    thresholds = sorted(list(set(thresholds)), reverse=True)\n",
    "    best_f1 = -1\n",
    "    best_threshold = thresholds[0] if len(thresholds) > 0 else 0.0\n",
    "    best_metrics = {}\n",
    "\n",
    "    true_labels = true_labels.astype(int)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (scores > threshold).astype(int)\n",
    "        TP = ((true_labels == 1) & (pred_labels == 1)).sum()\n",
    "        FP = ((true_labels == 0) & (pred_labels == 1)).sum()\n",
    "        TN = ((true_labels == 0) & (pred_labels == 0)).sum()\n",
    "        FN = ((true_labels == 1) & (pred_labels == 0)).sum()\n",
    "\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {'threshold': threshold, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "            \n",
    "    return best_threshold, best_metrics\n",
    "\n",
    "def process_hybrid(data_id):\n",
    "    print(f\"\\nProcessing: {data_id} (Hybrid SR + USAD)\")\n",
    "    \n",
    "    # Read data\n",
    "    train_df = pd.read_csv(f\"../../datasets/Tencent/train/{data_id}.csv\")\n",
    "    test_df = pd.read_csv(f\"../../datasets/Tencent/test/{data_id}.csv\")\n",
    "    \n",
    "    # Labels\n",
    "    test_labels = test_df[\"label\"].to_numpy().astype(int)\n",
    "    \n",
    "    # --- 1. SR on system_usage:0 ---\n",
    "    sr_col = \"system_usage:0\"\n",
    "    sr_scores = np.zeros(len(test_df))\n",
    "    \n",
    "    if sr_col in test_df.columns:\n",
    "        print(f\"  Running SR on {sr_col}...\")\n",
    "        sr_model = SpectralResidual(window_size=WINDOW_SIZE)\n",
    "        sr_scores_pl = sr_model.predict(pl.Series(test_df[sr_col].values))\n",
    "        sr_scores = sr_scores_pl.to_numpy()\n",
    "        sr_scores = np.nan_to_num(sr_scores, nan=0.0)\n",
    "    else:\n",
    "        print(f\"  Warning: {sr_col} missing. SR scores set to 0.\")\n",
    "\n",
    "    # --- 2. USAD on Rest ---\n",
    "    cols_exclude = [\"timestamp\", \"label\", sr_col]\n",
    "    train_usad = train_df.drop([c for c in cols_exclude if c in train_df.columns], axis=1).astype(float)\n",
    "    test_usad = test_df.drop([c for c in cols_exclude if c in test_df.columns], axis=1).astype(float)\n",
    "    \n",
    "    print(f\"  Running USAD on {train_usad.shape[1]} features...\")\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(train_usad.values)\n",
    "    x_test = scaler.transform(test_usad.values)\n",
    "    \n",
    "    # Windowing\n",
    "    def make_windows(data, ws):\n",
    "        n = data.shape[0]\n",
    "        if n <= ws: return np.empty((0, ws, data.shape[1]))\n",
    "        idx = np.arange(ws)[None, :] + np.arange(n - ws + 1)[:, None]\n",
    "        return data[idx]\n",
    "\n",
    "    train_win = make_windows(x_train, WINDOW_SIZE)\n",
    "    test_win = make_windows(x_test, WINDOW_SIZE)\n",
    "    \n",
    "    # Flatten\n",
    "    w_size = WINDOW_SIZE * x_train.shape[1]\n",
    "    z_size = WINDOW_SIZE * HIDDEN_SIZE\n",
    "    train_flat = train_win.reshape(-1, w_size)\n",
    "    test_flat = test_win.reshape(-1, w_size)\n",
    "    \n",
    "    # DataLoaders\n",
    "    split = int(0.8 * len(train_flat))\n",
    "    train_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(torch.from_numpy(train_flat[:split]).float()), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(torch.from_numpy(train_flat[split:]).float()), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(torch.from_numpy(test_flat).float()), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Train USAD\n",
    "    model_usad = USAD(N_EPOCHS, w_size, z_size).to(device())\n",
    "    model_usad.fit(train_loader, val_loader)\n",
    "    \n",
    "    # Predict USAD\n",
    "    res = model_usad.predict(test_loader)\n",
    "    if len(res) > 0:\n",
    "        usad_scores = torch.cat(res).cpu().numpy()\n",
    "    else:\n",
    "        usad_scores = np.array([])\n",
    "        \n",
    "    # --- 3. Combine ---\n",
    "    # Align lengths. USAD output length is N - W + 1.\n",
    "    valid_len = len(usad_scores)\n",
    "    start_idx = WINDOW_SIZE - 1\n",
    "    \n",
    "    # Slice SR scores and Labels to match USAD\n",
    "    sr_scores_aligned = sr_scores[start_idx : start_idx + valid_len]\n",
    "    labels_aligned = test_labels[start_idx : start_idx + valid_len]\n",
    "    \n",
    "    print(f\"  Aligned lengths: {valid_len}\")\n",
    "    \n",
    "    # Optimize Thresholds\n",
    "    print(\"  Optimizing SR threshold...\")\n",
    "    best_th_sr, metrics_sr = find_best_threshold(sr_scores_aligned, labels_aligned)\n",
    "    pred_sr = (sr_scores_aligned > best_th_sr).astype(int)\n",
    "    print(f\"    SR F1: {metrics_sr.get('f1', 0):.4f}\")\n",
    "    \n",
    "    print(\"  Optimizing USAD threshold...\")\n",
    "    best_th_usad, metrics_usad = find_best_threshold(usad_scores, labels_aligned)\n",
    "    pred_usad = (usad_scores > best_th_usad).astype(int)\n",
    "    print(f\"    USAD F1: {metrics_usad.get('f1', 0):.4f}\")\n",
    "    \n",
    "    # Logical OR\n",
    "    pred_final = (pred_sr | pred_usad).astype(int)\n",
    "    \n",
    "    # Calculate Final Metrics\n",
    "    TP = ((labels_aligned == 1) & (pred_final == 1)).sum()\n",
    "    FP = ((labels_aligned == 0) & (pred_final == 1)).sum()\n",
    "    TN = ((labels_aligned == 0) & (pred_final == 0)).sum()\n",
    "    FN = ((labels_aligned == 1) & (pred_final == 0)).sum()\n",
    "    \n",
    "    accuracy = (TP + TN) / len(labels_aligned) if len(labels_aligned) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"  Hybrid Result: F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}\")\n",
    "    \n",
    "    # Save Results\n",
    "    full_df = pd.concat([train_df, test_df])\n",
    "    complete_values = full_df.iloc[:, 1].to_numpy() # approx\n",
    "    complete_timestamps = range(len(complete_values)) # approx\n",
    "    \n",
    "    complete_predictions = np.zeros(len(complete_values))\n",
    "    pred_start_full = len(train_df) + WINDOW_SIZE - 1\n",
    "    end_idx_full = pred_start_full + valid_len\n",
    "    \n",
    "    complete_predictions[pred_start_full : end_idx_full] = pred_final\n",
    "    \n",
    "    res_df = pd.DataFrame({\n",
    "        'timestamp': complete_timestamps,\n",
    "        'value': complete_values,\n",
    "        'predicted': complete_predictions\n",
    "    })\n",
    "    res_df.to_csv(os.path.join(results_dir, f\"{data_id}.csv\"), index=False)\n",
    "    \n",
    "    return {\n",
    "        'id': data_id,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'sr_f1': metrics_sr.get('f1', 0),\n",
    "        'usad_f1': metrics_usad.get('f1', 0)\n",
    "    }\n",
    "\n",
    "# Run Loop\n",
    "results = []\n",
    "for i in selected_ids:\n",
    "    results.append(process_hybrid(i))\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary_df.round(4))\n",
    "summary_df.to_csv(os.path.join(results_dir, \"summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "yi9S0SGnDKNc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 3 datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 1\n",
      "  training model...\n",
      "Epoch [0], val_loss1: 0.2308, val_loss2: 0.2286\n",
      "Epoch [1], val_loss1: 0.2270, val_loss2: -0.0001\n",
      "Epoch [2], val_loss1: 0.2232, val_loss2: -0.0742\n",
      "Epoch [3], val_loss1: 0.2187, val_loss2: -0.1092\n",
      "Epoch [4], val_loss1: 0.2133, val_loss2: -0.1281\n",
      "Epoch [5], val_loss1: 0.2068, val_loss2: -0.1384\n",
      "Epoch [6], val_loss1: 0.1989, val_loss2: -0.1433\n",
      "Epoch [7], val_loss1: 0.1896, val_loss2: -0.1444\n",
      "Epoch [8], val_loss1: 0.1788, val_loss2: -0.1424\n",
      "Epoch [9], val_loss1: 0.1667, val_loss2: -0.1379\n",
      "Epoch [10], val_loss1: 0.1535, val_loss2: -0.1312\n",
      "Epoch [11], val_loss1: 0.1394, val_loss2: -0.1219\n",
      "Epoch [12], val_loss1: 0.1267, val_loss2: -0.1125\n",
      "Epoch [13], val_loss1: 0.1142, val_loss2: -0.1024\n",
      "Epoch [14], val_loss1: 0.1018, val_loss2: -0.0921\n",
      "Epoch [15], val_loss1: 0.0912, val_loss2: -0.0827\n",
      "Epoch [16], val_loss1: 0.0819, val_loss2: -0.0740\n",
      "Epoch [17], val_loss1: 0.0734, val_loss2: -0.0662\n",
      "Epoch [18], val_loss1: 0.0660, val_loss2: -0.0593\n",
      "Epoch [19], val_loss1: 0.0606, val_loss2: -0.0541\n",
      "Epoch [20], val_loss1: 0.0554, val_loss2: -0.0492\n",
      "Epoch [21], val_loss1: 0.0521, val_loss2: -0.0461\n",
      "Epoch [22], val_loss1: 0.0494, val_loss2: -0.0435\n",
      "Epoch [23], val_loss1: 0.0470, val_loss2: -0.0413\n",
      "Epoch [24], val_loss1: 0.0446, val_loss2: -0.0392\n",
      "Epoch [25], val_loss1: 0.0433, val_loss2: -0.0380\n",
      "Epoch [26], val_loss1: 0.0420, val_loss2: -0.0368\n",
      "Epoch [27], val_loss1: 0.0407, val_loss2: -0.0356\n",
      "Epoch [28], val_loss1: 0.0398, val_loss2: -0.0348\n",
      "Epoch [29], val_loss1: 0.0389, val_loss2: -0.0341\n",
      "Epoch [30], val_loss1: 0.0379, val_loss2: -0.0331\n",
      "Epoch [31], val_loss1: 0.0368, val_loss2: -0.0322\n",
      "Epoch [32], val_loss1: 0.0359, val_loss2: -0.0314\n",
      "Epoch [33], val_loss1: 0.0349, val_loss2: -0.0306\n",
      "Epoch [34], val_loss1: 0.0337, val_loss2: -0.0296\n",
      "Epoch [35], val_loss1: 0.0322, val_loss2: -0.0284\n",
      "Epoch [36], val_loss1: 0.0309, val_loss2: -0.0273\n",
      "Epoch [37], val_loss1: 0.0302, val_loss2: -0.0267\n",
      "Epoch [38], val_loss1: 0.0304, val_loss2: -0.0271\n",
      "Epoch [39], val_loss1: 0.0316, val_loss2: -0.0284\n",
      "Epoch [40], val_loss1: 0.0334, val_loss2: -0.0303\n",
      "Epoch [41], val_loss1: 0.0353, val_loss2: -0.0322\n",
      "Epoch [42], val_loss1: 0.0366, val_loss2: -0.0336\n",
      "Epoch [43], val_loss1: 0.0372, val_loss2: -0.0343\n",
      "Epoch [44], val_loss1: 0.0370, val_loss2: -0.0342\n",
      "Epoch [45], val_loss1: 0.0364, val_loss2: -0.0338\n",
      "Epoch [46], val_loss1: 0.0357, val_loss2: -0.0332\n",
      "Epoch [47], val_loss1: 0.0351, val_loss2: -0.0327\n",
      "Epoch [48], val_loss1: 0.0349, val_loss2: -0.0326\n",
      "Epoch [49], val_loss1: 0.0349, val_loss2: -0.0327\n",
      "Epoch [50], val_loss1: 0.0353, val_loss2: -0.0331\n",
      "Epoch [51], val_loss1: 0.0357, val_loss2: -0.0336\n",
      "Epoch [52], val_loss1: 0.0361, val_loss2: -0.0341\n",
      "Epoch [53], val_loss1: 0.0363, val_loss2: -0.0344\n",
      "Epoch [54], val_loss1: 0.0364, val_loss2: -0.0345\n",
      "Epoch [55], val_loss1: 0.0362, val_loss2: -0.0344\n",
      "Epoch [56], val_loss1: 0.0359, val_loss2: -0.0342\n",
      "Epoch [57], val_loss1: 0.0358, val_loss2: -0.0341\n",
      "Epoch [58], val_loss1: 0.0357, val_loss2: -0.0341\n",
      "Epoch [59], val_loss1: 0.0358, val_loss2: -0.0342\n",
      "Epoch [60], val_loss1: 0.0359, val_loss2: -0.0343\n",
      "Epoch [61], val_loss1: 0.0361, val_loss2: -0.0345\n",
      "Epoch [62], val_loss1: 0.0362, val_loss2: -0.0347\n",
      "Epoch [63], val_loss1: 0.0363, val_loss2: -0.0349\n",
      "Epoch [64], val_loss1: 0.0364, val_loss2: -0.0350\n",
      "Epoch [65], val_loss1: 0.0363, val_loss2: -0.0349\n",
      "Epoch [66], val_loss1: 0.0362, val_loss2: -0.0349\n",
      "Epoch [67], val_loss1: 0.0363, val_loss2: -0.0350\n",
      "Epoch [68], val_loss1: 0.0364, val_loss2: -0.0351\n",
      "Epoch [69], val_loss1: 0.0365, val_loss2: -0.0353\n",
      "Epoch [70], val_loss1: 0.0366, val_loss2: -0.0354\n",
      "Epoch [71], val_loss1: 0.0366, val_loss2: -0.0354\n",
      "Epoch [72], val_loss1: 0.0366, val_loss2: -0.0355\n",
      "Epoch [73], val_loss1: 0.0366, val_loss2: -0.0354\n",
      "Epoch [74], val_loss1: 0.0365, val_loss2: -0.0354\n",
      "Epoch [75], val_loss1: 0.0364, val_loss2: -0.0354\n",
      "Epoch [76], val_loss1: 0.0364, val_loss2: -0.0354\n",
      "Epoch [77], val_loss1: 0.0364, val_loss2: -0.0354\n",
      "Epoch [78], val_loss1: 0.0365, val_loss2: -0.0355\n",
      "Epoch [79], val_loss1: 0.0365, val_loss2: -0.0355\n",
      "Epoch [80], val_loss1: 0.0365, val_loss2: -0.0355\n",
      "Epoch [81], val_loss1: 0.0365, val_loss2: -0.0355\n",
      "Epoch [82], val_loss1: 0.0365, val_loss2: -0.0355\n",
      "Epoch [83], val_loss1: 0.0364, val_loss2: -0.0355\n",
      "Epoch [84], val_loss1: 0.0365, val_loss2: -0.0356\n",
      "Epoch [85], val_loss1: 0.0365, val_loss2: -0.0357\n",
      "Epoch [86], val_loss1: 0.0366, val_loss2: -0.0357\n",
      "Epoch [87], val_loss1: 0.0366, val_loss2: -0.0358\n",
      "Epoch [88], val_loss1: 0.0366, val_loss2: -0.0358\n",
      "Epoch [89], val_loss1: 0.0366, val_loss2: -0.0358\n",
      "Epoch [90], val_loss1: 0.0366, val_loss2: -0.0358\n",
      "Epoch [91], val_loss1: 0.0366, val_loss2: -0.0358\n",
      "Epoch [92], val_loss1: 0.0366, val_loss2: -0.0358\n",
      "Epoch [93], val_loss1: 0.0366, val_loss2: -0.0359\n",
      "Epoch [94], val_loss1: 0.0366, val_loss2: -0.0359\n",
      "Epoch [95], val_loss1: 0.0365, val_loss2: -0.0358\n",
      "Epoch [96], val_loss1: 0.0365, val_loss2: -0.0358\n",
      "Epoch [97], val_loss1: 0.0365, val_loss2: -0.0358\n",
      "Epoch [98], val_loss1: 0.0364, val_loss2: -0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:  33%|███▎      | 1/3 [00:14<00:28, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99], val_loss1: 0.0363, val_loss2: -0.0356\n",
      "  anomaly detection...\n",
      "  Scores stats: min=0.052722, max=726000.250000, mean=2218.185059\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/usad/1.csv\n",
      "  ID 1 completed:\n",
      "    best_threshold: 0.4465\n",
      "    f1: 0.8627\n",
      "    precision: 0.7586\n",
      "    recall: 1.0000\n",
      "    accuracy: 0.9951\n",
      "processing: 2\n",
      "  training model...\n",
      "Epoch [0], val_loss1: 0.2307, val_loss2: 0.2337\n",
      "Epoch [1], val_loss1: 0.2290, val_loss2: 0.0000\n",
      "Epoch [2], val_loss1: 0.2259, val_loss2: -0.0755\n",
      "Epoch [3], val_loss1: 0.2218, val_loss2: -0.1110\n",
      "Epoch [4], val_loss1: 0.2165, val_loss2: -0.1299\n",
      "Epoch [5], val_loss1: 0.2096, val_loss2: -0.1398\n",
      "Epoch [6], val_loss1: 0.2014, val_loss2: -0.1439\n",
      "Epoch [7], val_loss1: 0.1917, val_loss2: -0.1439\n",
      "Epoch [8], val_loss1: 0.1807, val_loss2: -0.1409\n",
      "Epoch [9], val_loss1: 0.1685, val_loss2: -0.1356\n",
      "Epoch [10], val_loss1: 0.1553, val_loss2: -0.1284\n",
      "Epoch [11], val_loss1: 0.1415, val_loss2: -0.1198\n",
      "Epoch [12], val_loss1: 0.1281, val_loss2: -0.1104\n",
      "Epoch [13], val_loss1: 0.1154, val_loss2: -0.1006\n",
      "Epoch [14], val_loss1: 0.1038, val_loss2: -0.0910\n",
      "Epoch [15], val_loss1: 0.0933, val_loss2: -0.0820\n",
      "Epoch [16], val_loss1: 0.0842, val_loss2: -0.0740\n",
      "Epoch [17], val_loss1: 0.0762, val_loss2: -0.0669\n",
      "Epoch [18], val_loss1: 0.0691, val_loss2: -0.0606\n",
      "Epoch [19], val_loss1: 0.0634, val_loss2: -0.0555\n",
      "Epoch [20], val_loss1: 0.0586, val_loss2: -0.0512\n",
      "Epoch [21], val_loss1: 0.0545, val_loss2: -0.0476\n",
      "Epoch [22], val_loss1: 0.0511, val_loss2: -0.0446\n",
      "Epoch [23], val_loss1: 0.0479, val_loss2: -0.0419\n",
      "Epoch [24], val_loss1: 0.0454, val_loss2: -0.0398\n",
      "Epoch [25], val_loss1: 0.0432, val_loss2: -0.0379\n",
      "Epoch [26], val_loss1: 0.0415, val_loss2: -0.0364\n",
      "Epoch [27], val_loss1: 0.0398, val_loss2: -0.0349\n",
      "Epoch [28], val_loss1: 0.0378, val_loss2: -0.0332\n",
      "Epoch [29], val_loss1: 0.0355, val_loss2: -0.0310\n",
      "Epoch [30], val_loss1: 0.0327, val_loss2: -0.0285\n",
      "Epoch [31], val_loss1: 0.0299, val_loss2: -0.0260\n",
      "Epoch [32], val_loss1: 0.0272, val_loss2: -0.0236\n",
      "Epoch [33], val_loss1: 0.0257, val_loss2: -0.0224\n",
      "Epoch [34], val_loss1: 0.0263, val_loss2: -0.0231\n",
      "Epoch [35], val_loss1: 0.0289, val_loss2: -0.0258\n",
      "Epoch [36], val_loss1: 0.0325, val_loss2: -0.0294\n",
      "Epoch [37], val_loss1: 0.0357, val_loss2: -0.0326\n",
      "Epoch [38], val_loss1: 0.0375, val_loss2: -0.0344\n",
      "Epoch [39], val_loss1: 0.0378, val_loss2: -0.0348\n",
      "Epoch [40], val_loss1: 0.0370, val_loss2: -0.0342\n",
      "Epoch [41], val_loss1: 0.0356, val_loss2: -0.0330\n",
      "Epoch [42], val_loss1: 0.0342, val_loss2: -0.0318\n",
      "Epoch [43], val_loss1: 0.0331, val_loss2: -0.0309\n",
      "Epoch [44], val_loss1: 0.0326, val_loss2: -0.0305\n",
      "Epoch [45], val_loss1: 0.0327, val_loss2: -0.0306\n",
      "Epoch [46], val_loss1: 0.0332, val_loss2: -0.0312\n",
      "Epoch [47], val_loss1: 0.0339, val_loss2: -0.0320\n",
      "Epoch [48], val_loss1: 0.0345, val_loss2: -0.0326\n",
      "Epoch [49], val_loss1: 0.0348, val_loss2: -0.0330\n",
      "Epoch [50], val_loss1: 0.0349, val_loss2: -0.0331\n",
      "Epoch [51], val_loss1: 0.0347, val_loss2: -0.0330\n",
      "Epoch [52], val_loss1: 0.0345, val_loss2: -0.0329\n",
      "Epoch [53], val_loss1: 0.0344, val_loss2: -0.0328\n",
      "Epoch [54], val_loss1: 0.0344, val_loss2: -0.0328\n",
      "Epoch [55], val_loss1: 0.0344, val_loss2: -0.0329\n",
      "Epoch [56], val_loss1: 0.0344, val_loss2: -0.0330\n",
      "Epoch [57], val_loss1: 0.0345, val_loss2: -0.0331\n",
      "Epoch [58], val_loss1: 0.0345, val_loss2: -0.0332\n",
      "Epoch [59], val_loss1: 0.0346, val_loss2: -0.0332\n",
      "Epoch [60], val_loss1: 0.0346, val_loss2: -0.0333\n",
      "Epoch [61], val_loss1: 0.0346, val_loss2: -0.0333\n",
      "Epoch [62], val_loss1: 0.0346, val_loss2: -0.0334\n",
      "Epoch [63], val_loss1: 0.0347, val_loss2: -0.0335\n",
      "Epoch [64], val_loss1: 0.0347, val_loss2: -0.0335\n",
      "Epoch [65], val_loss1: 0.0347, val_loss2: -0.0336\n",
      "Epoch [66], val_loss1: 0.0348, val_loss2: -0.0336\n",
      "Epoch [67], val_loss1: 0.0348, val_loss2: -0.0336\n",
      "Epoch [68], val_loss1: 0.0348, val_loss2: -0.0337\n",
      "Epoch [69], val_loss1: 0.0348, val_loss2: -0.0337\n",
      "Epoch [70], val_loss1: 0.0348, val_loss2: -0.0337\n",
      "Epoch [71], val_loss1: 0.0348, val_loss2: -0.0338\n",
      "Epoch [72], val_loss1: 0.0349, val_loss2: -0.0338\n",
      "Epoch [73], val_loss1: 0.0349, val_loss2: -0.0339\n",
      "Epoch [74], val_loss1: 0.0349, val_loss2: -0.0339\n",
      "Epoch [75], val_loss1: 0.0350, val_loss2: -0.0340\n",
      "Epoch [76], val_loss1: 0.0350, val_loss2: -0.0340\n",
      "Epoch [77], val_loss1: 0.0349, val_loss2: -0.0340\n",
      "Epoch [78], val_loss1: 0.0348, val_loss2: -0.0338\n",
      "Epoch [79], val_loss1: 0.0347, val_loss2: -0.0338\n",
      "Epoch [80], val_loss1: 0.0349, val_loss2: -0.0340\n",
      "Epoch [81], val_loss1: 0.0350, val_loss2: -0.0341\n",
      "Epoch [82], val_loss1: 0.0350, val_loss2: -0.0342\n",
      "Epoch [83], val_loss1: 0.0350, val_loss2: -0.0342\n",
      "Epoch [84], val_loss1: 0.0350, val_loss2: -0.0342\n",
      "Epoch [85], val_loss1: 0.0350, val_loss2: -0.0342\n",
      "Epoch [86], val_loss1: 0.0350, val_loss2: -0.0342\n",
      "Epoch [87], val_loss1: 0.0350, val_loss2: -0.0343\n",
      "Epoch [88], val_loss1: 0.0351, val_loss2: -0.0343\n",
      "Epoch [89], val_loss1: 0.0351, val_loss2: -0.0343\n",
      "Epoch [90], val_loss1: 0.0351, val_loss2: -0.0344\n",
      "Epoch [91], val_loss1: 0.0351, val_loss2: -0.0344\n",
      "Epoch [92], val_loss1: 0.0351, val_loss2: -0.0344\n",
      "Epoch [93], val_loss1: 0.0351, val_loss2: -0.0345\n",
      "Epoch [94], val_loss1: 0.0351, val_loss2: -0.0345\n",
      "Epoch [95], val_loss1: 0.0351, val_loss2: -0.0345\n",
      "Epoch [96], val_loss1: 0.0351, val_loss2: -0.0344\n",
      "Epoch [97], val_loss1: 0.0351, val_loss2: -0.0344\n",
      "Epoch [98], val_loss1: 0.0351, val_loss2: -0.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:  67%|██████▋   | 2/3 [00:26<00:12, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99], val_loss1: 0.0351, val_loss2: -0.0345\n",
      "  anomaly detection...\n",
      "  Scores stats: min=0.048963, max=1.011518, mean=0.213491\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/usad/2.csv\n",
      "  ID 2 completed:\n",
      "    best_threshold: 0.4389\n",
      "    f1: 0.3038\n",
      "    precision: 0.9231\n",
      "    recall: 0.1818\n",
      "    accuracy: 0.9872\n",
      "processing: 3\n",
      "  training model...\n",
      "Epoch [0], val_loss1: 0.2288, val_loss2: 0.2313\n",
      "Epoch [1], val_loss1: 0.2273, val_loss2: -0.0000\n",
      "Epoch [2], val_loss1: 0.2244, val_loss2: -0.0751\n",
      "Epoch [3], val_loss1: 0.2203, val_loss2: -0.1103\n",
      "Epoch [4], val_loss1: 0.2149, val_loss2: -0.1291\n",
      "Epoch [5], val_loss1: 0.2082, val_loss2: -0.1390\n",
      "Epoch [6], val_loss1: 0.2000, val_loss2: -0.1432\n",
      "Epoch [7], val_loss1: 0.1905, val_loss2: -0.1434\n",
      "Epoch [8], val_loss1: 0.1797, val_loss2: -0.1406\n",
      "Epoch [9], val_loss1: 0.1675, val_loss2: -0.1353\n",
      "Epoch [10], val_loss1: 0.1546, val_loss2: -0.1280\n",
      "Epoch [11], val_loss1: 0.1410, val_loss2: -0.1192\n",
      "Epoch [12], val_loss1: 0.1264, val_loss2: -0.1088\n",
      "Epoch [13], val_loss1: 0.1117, val_loss2: -0.0975\n",
      "Epoch [14], val_loss1: 0.0973, val_loss2: -0.0859\n",
      "Epoch [15], val_loss1: 0.0839, val_loss2: -0.0745\n",
      "Epoch [16], val_loss1: 0.0717, val_loss2: -0.0638\n",
      "Epoch [17], val_loss1: 0.0621, val_loss2: -0.0552\n",
      "Epoch [18], val_loss1: 0.0545, val_loss2: -0.0482\n",
      "Epoch [19], val_loss1: 0.0485, val_loss2: -0.0428\n",
      "Epoch [20], val_loss1: 0.0440, val_loss2: -0.0386\n",
      "Epoch [21], val_loss1: 0.0409, val_loss2: -0.0359\n",
      "Epoch [22], val_loss1: 0.0389, val_loss2: -0.0341\n",
      "Epoch [23], val_loss1: 0.0375, val_loss2: -0.0329\n",
      "Epoch [24], val_loss1: 0.0368, val_loss2: -0.0324\n",
      "Epoch [25], val_loss1: 0.0363, val_loss2: -0.0321\n",
      "Epoch [26], val_loss1: 0.0360, val_loss2: -0.0319\n",
      "Epoch [27], val_loss1: 0.0358, val_loss2: -0.0319\n",
      "Epoch [28], val_loss1: 0.0358, val_loss2: -0.0321\n",
      "Epoch [29], val_loss1: 0.0365, val_loss2: -0.0329\n",
      "Epoch [30], val_loss1: 0.0376, val_loss2: -0.0341\n",
      "Epoch [31], val_loss1: 0.0376, val_loss2: -0.0342\n",
      "Epoch [32], val_loss1: 0.0360, val_loss2: -0.0328\n",
      "Epoch [33], val_loss1: 0.0330, val_loss2: -0.0302\n",
      "Epoch [34], val_loss1: 0.0306, val_loss2: -0.0280\n",
      "Epoch [35], val_loss1: 0.0307, val_loss2: -0.0283\n",
      "Epoch [36], val_loss1: 0.0335, val_loss2: -0.0310\n",
      "Epoch [37], val_loss1: 0.0369, val_loss2: -0.0344\n",
      "Epoch [38], val_loss1: 0.0388, val_loss2: -0.0363\n",
      "Epoch [39], val_loss1: 0.0386, val_loss2: -0.0362\n",
      "Epoch [40], val_loss1: 0.0371, val_loss2: -0.0349\n",
      "Epoch [41], val_loss1: 0.0357, val_loss2: -0.0336\n",
      "Epoch [42], val_loss1: 0.0351, val_loss2: -0.0332\n",
      "Epoch [43], val_loss1: 0.0356, val_loss2: -0.0337\n",
      "Epoch [44], val_loss1: 0.0365, val_loss2: -0.0347\n",
      "Epoch [45], val_loss1: 0.0374, val_loss2: -0.0356\n",
      "Epoch [46], val_loss1: 0.0377, val_loss2: -0.0359\n",
      "Epoch [47], val_loss1: 0.0375, val_loss2: -0.0357\n",
      "Epoch [48], val_loss1: 0.0369, val_loss2: -0.0353\n",
      "Epoch [49], val_loss1: 0.0365, val_loss2: -0.0349\n",
      "Epoch [50], val_loss1: 0.0364, val_loss2: -0.0348\n",
      "Epoch [51], val_loss1: 0.0366, val_loss2: -0.0351\n",
      "Epoch [52], val_loss1: 0.0369, val_loss2: -0.0354\n",
      "Epoch [53], val_loss1: 0.0371, val_loss2: -0.0357\n",
      "Epoch [54], val_loss1: 0.0372, val_loss2: -0.0358\n",
      "Epoch [55], val_loss1: 0.0371, val_loss2: -0.0357\n",
      "Epoch [56], val_loss1: 0.0370, val_loss2: -0.0356\n",
      "Epoch [57], val_loss1: 0.0368, val_loss2: -0.0355\n",
      "Epoch [58], val_loss1: 0.0369, val_loss2: -0.0356\n",
      "Epoch [59], val_loss1: 0.0369, val_loss2: -0.0357\n",
      "Epoch [60], val_loss1: 0.0370, val_loss2: -0.0358\n",
      "Epoch [61], val_loss1: 0.0370, val_loss2: -0.0358\n",
      "Epoch [62], val_loss1: 0.0369, val_loss2: -0.0357\n",
      "Epoch [63], val_loss1: 0.0368, val_loss2: -0.0356\n",
      "Epoch [64], val_loss1: 0.0367, val_loss2: -0.0355\n",
      "Epoch [65], val_loss1: 0.0367, val_loss2: -0.0355\n",
      "Epoch [66], val_loss1: 0.0367, val_loss2: -0.0356\n",
      "Epoch [67], val_loss1: 0.0368, val_loss2: -0.0357\n",
      "Epoch [68], val_loss1: 0.0369, val_loss2: -0.0358\n",
      "Epoch [69], val_loss1: 0.0369, val_loss2: -0.0358\n",
      "Epoch [70], val_loss1: 0.0368, val_loss2: -0.0358\n",
      "Epoch [71], val_loss1: 0.0368, val_loss2: -0.0358\n",
      "Epoch [72], val_loss1: 0.0367, val_loss2: -0.0358\n",
      "Epoch [73], val_loss1: 0.0367, val_loss2: -0.0357\n",
      "Epoch [74], val_loss1: 0.0367, val_loss2: -0.0357\n",
      "Epoch [75], val_loss1: 0.0367, val_loss2: -0.0357\n",
      "Epoch [76], val_loss1: 0.0367, val_loss2: -0.0357\n",
      "Epoch [77], val_loss1: 0.0366, val_loss2: -0.0357\n",
      "Epoch [78], val_loss1: 0.0366, val_loss2: -0.0357\n",
      "Epoch [79], val_loss1: 0.0365, val_loss2: -0.0356\n",
      "Epoch [80], val_loss1: 0.0364, val_loss2: -0.0356\n",
      "Epoch [81], val_loss1: 0.0364, val_loss2: -0.0355\n",
      "Epoch [82], val_loss1: 0.0364, val_loss2: -0.0355\n",
      "Epoch [83], val_loss1: 0.0364, val_loss2: -0.0356\n",
      "Epoch [84], val_loss1: 0.0364, val_loss2: -0.0356\n",
      "Epoch [85], val_loss1: 0.0363, val_loss2: -0.0355\n",
      "Epoch [86], val_loss1: 0.0362, val_loss2: -0.0354\n",
      "Epoch [87], val_loss1: 0.0362, val_loss2: -0.0354\n",
      "Epoch [88], val_loss1: 0.0361, val_loss2: -0.0354\n",
      "Epoch [89], val_loss1: 0.0361, val_loss2: -0.0354\n",
      "Epoch [90], val_loss1: 0.0362, val_loss2: -0.0354\n",
      "Epoch [91], val_loss1: 0.0362, val_loss2: -0.0354\n",
      "Epoch [92], val_loss1: 0.0362, val_loss2: -0.0355\n",
      "Epoch [93], val_loss1: 0.0362, val_loss2: -0.0354\n",
      "Epoch [94], val_loss1: 0.0361, val_loss2: -0.0354\n",
      "Epoch [95], val_loss1: 0.0360, val_loss2: -0.0353\n",
      "Epoch [96], val_loss1: 0.0360, val_loss2: -0.0353\n",
      "Epoch [97], val_loss1: 0.0359, val_loss2: -0.0352\n",
      "Epoch [98], val_loss1: 0.0359, val_loss2: -0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 3/3 [00:37<00:00, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99], val_loss1: 0.0360, val_loss2: -0.0353\n",
      "  anomaly detection...\n",
      "  Scores stats: min=0.052601, max=1.022958, mean=0.222413\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/usad/3.csv\n",
      "  ID 3 completed:\n",
      "    best_threshold: 1.0229\n",
      "    f1: 0.0000\n",
      "    precision: 0.0000\n",
      "    recall: 0.0000\n",
      "    accuracy: 0.9998\n",
      "summary results saved to: ../../results/models/usad/usad.csv\n",
      "\n",
      "================================================================================\n",
      "USAD anomaly detection results\n",
      "================================================================================\n",
      "processed 3 datasets\n",
      "average F1: 0.3888 ± 0.4376\n",
      "average precision: 0.5606 ± 0.4924\n",
      "average recall: 0.3939 ± 0.5327\n",
      "average accuracy: 0.9940 ± 0.0063\n",
      "average training time: 12.36s\n",
      "average scoring time: 0.03s\n",
      "================================================================================\n",
      "details:\n",
      "   id      f1  precision  recall  accuracy  best_threshold  tp  fp    tn  fn\n",
      "0   1  0.8627     0.7586  1.0000    0.9951          0.4465  66  21  4222   0\n",
      "1   2  0.3038     0.9231  0.1818    0.9872          0.4389  12   1  4242  54\n",
      "2   3  0.0000     0.0000  0.0000    0.9998          1.0229   0   1  4308   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 100\n",
    "HIDDEN_SIZE = 100\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "results_dir = \"../../results/models/usad\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "selected_ids = [1, 2, 3]\n",
    "\n",
    "def find_best_threshold(scores, true_labels, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        # Handle case where scores are all same (e.g. 0)\n",
    "        if np.min(scores) == np.max(scores):\n",
    "            thresholds = [scores[0]]\n",
    "        else:\n",
    "            thresholds = [np.percentile(scores, p) for p in range(0, 90, 5)]\n",
    "            thresholds.extend([np.percentile(scores, p) for p in range(90, 100, 1)])\n",
    "            thresholds.extend([np.percentile(scores, p) for p in [99.1, 99.3, 99.5, 99.7, 99.9, 99.95, 99.99]])\n",
    "            \n",
    "    thresholds = sorted(list(set(thresholds)), reverse=True)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_threshold = thresholds[0] if len(thresholds) > 0 else 0.0\n",
    "    \n",
    "    best_metrics = {\n",
    "        'threshold': best_threshold,\n",
    "        'accuracy': 0.0,\n",
    "        'precision': 0.0,\n",
    "        'recall': 0.0,\n",
    "        'f1': 0.0,\n",
    "        'fnr': 0.0,\n",
    "        'fpr': 0.0,\n",
    "        'tp': 0,\n",
    "        'fp': 0,\n",
    "        'tn': 0,\n",
    "        'fn': 0\n",
    "    }\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (scores > threshold).astype(int)\n",
    "        TP = ((true_labels == 1) & (pred_labels == 1)).sum()\n",
    "        FP = ((true_labels == 0) & (pred_labels == 1)).sum()\n",
    "        TN = ((true_labels == 0) & (pred_labels == 0)).sum()\n",
    "        FN = ((true_labels == 1) & (pred_labels == 0)).sum()\n",
    "\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                'threshold': threshold,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'fnr': fnr,\n",
    "                'fpr': fpr,\n",
    "                'tp': TP,\n",
    "                'fp': FP,\n",
    "                'tn': TN,\n",
    "                'fn': FN\n",
    "            }\n",
    "            \n",
    "    return best_threshold, best_metrics\n",
    "\n",
    "def process_single_id(data_id):\n",
    "    print(f\"processing: {data_id}\")\n",
    "    \n",
    "    # Read data\n",
    "    train_df_raw = pd.read_csv(f\"../../datasets/Tencent/train/{data_id}.csv\")\n",
    "    test_df_raw = pd.read_csv(f\"../../datasets/Tencent/test/{data_id}.csv\")\n",
    "    \n",
    "    # Prepare Train Data\n",
    "    train_vals = train_df_raw.drop([\"timestamp\", \"label\"], axis=1, errors='ignore')\n",
    "    train_vals = train_vals.astype(float)\n",
    "    \n",
    "    # Prepare Test Data\n",
    "    test_vals = test_df_raw.drop([\"timestamp\", \"label\"], axis=1, errors='ignore')\n",
    "    test_vals = test_vals.astype(float)\n",
    "    \n",
    "    # Normalization\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_train = min_max_scaler.fit_transform(train_vals.values)\n",
    "    x_test = min_max_scaler.transform(test_vals.values)\n",
    "    \n",
    "    # Create Windows\n",
    "    def make_windows(data_arr):\n",
    "        # data_arr: (N, F)\n",
    "        n = data_arr.shape[0]\n",
    "        if n <= WINDOW_SIZE:\n",
    "            return np.empty((0, WINDOW_SIZE, data_arr.shape[1]))\n",
    "        indexer = np.arange(WINDOW_SIZE)[None, :] + np.arange(n - WINDOW_SIZE + 1)[:, None]\n",
    "        return data_arr[indexer]\n",
    "\n",
    "    train_windows = make_windows(x_train)\n",
    "    test_windows = make_windows(x_test)\n",
    "    \n",
    "    # Flatten windows for USAD\n",
    "    w_size = WINDOW_SIZE * x_train.shape[1]\n",
    "    z_size = WINDOW_SIZE * HIDDEN_SIZE\n",
    "    \n",
    "    train_windows_flat = train_windows.reshape(-1, w_size)\n",
    "    test_windows_flat = test_windows.reshape(-1, w_size)\n",
    "    \n",
    "    # Split Train/Val\n",
    "    split_idx = int(0.8 * len(train_windows_flat))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        data_utils.TensorDataset(\n",
    "            torch.from_numpy(train_windows_flat[:split_idx]).float()\n",
    "        ),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        data_utils.TensorDataset(\n",
    "            torch.from_numpy(train_windows_flat[split_idx:]).float()\n",
    "        ),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        data_utils.TensorDataset(\n",
    "            torch.from_numpy(test_windows_flat).float()\n",
    "        ),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model = USAD(N_EPOCHS, w_size, z_size).to(device())\n",
    "    \n",
    "    print(\"  training model...\")\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_loader, val_loader)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"  anomaly detection...\")\n",
    "    start_time = time.time()\n",
    "    results_list = model.predict(test_loader)\n",
    "    if len(results_list) > 0:\n",
    "        scores = torch.cat(results_list).cpu().numpy()\n",
    "    else:\n",
    "        scores = np.array([])\n",
    "    scoring_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Scores stats: min={scores.min():.6f}, max={scores.max():.6f}, mean={scores.mean():.6f}\")\n",
    "    \n",
    "    # Window-based Labeling Logic\n",
    "    test_true_labels = test_df_raw[\"label\"].to_numpy()\n",
    "    \n",
    "    # Create windows of labels using the same make_windows function\n",
    "    # We reshape labels to (N, 1) then flatten the result to (num_windows, WINDOW_SIZE)\n",
    "    label_windows = make_windows(test_true_labels.reshape(-1, 1)).reshape(-1, WINDOW_SIZE)\n",
    "    \n",
    "    # If ANY point in window is anomaly (1), then window label is 1\n",
    "    y_test = (label_windows.sum(axis=1) > 0).astype(int)\n",
    "    \n",
    "    # Align lengths\n",
    "    min_len = min(len(scores), len(y_test))\n",
    "    scores = scores[:min_len]\n",
    "    y_test = y_test[:min_len]\n",
    "\n",
    "    print(\"  finding best threshold...\")\n",
    "    best_threshold, best_metrics = find_best_threshold(scores, y_test)\n",
    "    \n",
    "    # Save Results (Complete Data)\n",
    "    full_df = pd.concat([train_df_raw, test_df_raw])\n",
    "    \n",
    "    if \"value\" in full_df.columns:\n",
    "        complete_values = full_df[\"value\"].to_numpy()\n",
    "    else:\n",
    "        complete_values = full_df.iloc[:, 1].to_numpy() # Use first feature column\n",
    "        \n",
    "    if \"label\" in full_df.columns:\n",
    "        complete_labels = full_df[\"label\"].to_numpy()\n",
    "    else:\n",
    "        complete_labels = np.zeros(len(complete_values))\n",
    "        \n",
    "    if \"timestamp\" in full_df.columns:\n",
    "        complete_timestamps = full_df[\"timestamp\"].to_numpy()\n",
    "    else:\n",
    "        complete_timestamps = range(len(complete_values))\n",
    "        \n",
    "    complete_predictions = np.zeros(len(complete_values))\n",
    "    complete_anomaly_scores = np.zeros(len(complete_values))\n",
    "    \n",
    "    # Align predictions\n",
    "    # Test scores correspond to windows. \n",
    "    # We align the score to the END of the window (point-based alignment for visualization)\n",
    "    # Start index in full_df: len(train) + WINDOW_SIZE - 1\n",
    "    \n",
    "    pred_start_idx = len(train_df_raw) + WINDOW_SIZE - 1\n",
    "    \n",
    "    end_idx = pred_start_idx + len(scores)\n",
    "    if end_idx > len(complete_values):\n",
    "        end_idx = len(complete_values)\n",
    "        scores = scores[:end_idx - pred_start_idx]\n",
    "        \n",
    "    complete_predictions[pred_start_idx:end_idx] = (scores > best_threshold).astype(int)\n",
    "    complete_anomaly_scores[pred_start_idx:end_idx] = scores\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "        'timestamp': complete_timestamps,\n",
    "        'value': complete_values,\n",
    "        'label': complete_labels,\n",
    "        'predicted': complete_predictions,\n",
    "        'anomaly_score': complete_anomaly_scores\n",
    "    })\n",
    "    \n",
    "    output_file = os.path.join(results_dir, f\"{data_id}.csv\")\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"  results saved to: {output_file}\")\n",
    "    \n",
    "    return {\n",
    "        'id': data_id,\n",
    "        'training_time': training_time,\n",
    "        'testing_time': scoring_time,\n",
    "        'total_time': training_time + scoring_time,\n",
    "        'train_samples': len(train_df_raw),\n",
    "        'test_samples': len(test_df_raw),\n",
    "        'best_threshold': best_threshold,\n",
    "        **best_metrics\n",
    "    }\n",
    "\n",
    "# Main Loop\n",
    "all_results = []\n",
    "print(f\"processing {len(selected_ids)} datasets...\")\n",
    "\n",
    "for data_id in tqdm(selected_ids, desc=\"processing\"):\n",
    "    try:\n",
    "        result = process_single_id(data_id)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        print(f\"  ID {data_id} completed:\")\n",
    "        print(f\"    best_threshold: {result['best_threshold']:.4f}\")\n",
    "        print(f\"    f1: {result['f1']:.4f}\")\n",
    "        print(f\"    precision: {result['precision']:.4f}\")\n",
    "        print(f\"    recall: {result['recall']:.4f}\")\n",
    "        print(f\"    accuracy: {result['accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  processing {data_id} failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "if all_results:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    summary_file = os.path.join(results_dir, \"usad.csv\")\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"summary results saved to: {summary_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"USAD anomaly detection results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"processed {len(all_results)} datasets\")\n",
    "    print(f\"average F1: {summary_df['f1'].mean():.4f} ± {summary_df['f1'].std():.4f}\")\n",
    "    print(f\"average precision: {summary_df['precision'].mean():.4f} ± {summary_df['precision'].std():.4f}\")\n",
    "    print(f\"average recall: {summary_df['recall'].mean():.4f} ± {summary_df['recall'].std():.4f}\")\n",
    "    print(f\"average accuracy: {summary_df['accuracy'].mean():.4f} ± {summary_df['accuracy'].std():.4f}\")\n",
    "    print(f\"average training time: {summary_df['training_time'].mean():.2f}s\")\n",
    "    print(f\"average scoring time: {summary_df['testing_time'].mean():.2f}s\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"details:\")\n",
    "    display_cols = ['id', 'f1', 'precision', 'recall', 'accuracy', 'best_threshold', 'tp', 'fp', 'tn', 'fn']\n",
    "    print(summary_df[display_cols].round(4))\n",
    "else:\n",
    "    print(\"no results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing Synthetic Dataset (USAD)...\n",
      "========================================\n",
      "  Train shape: (2500, 12)\n",
      "  Test shape: (2500, 12)\n",
      "  training model...\n",
      "Epoch [0], val_loss1: 0.0418, val_loss2: 0.0413\n",
      "Epoch [1], val_loss1: 0.0419, val_loss2: -0.0000\n",
      "Epoch [2], val_loss1: 0.0422, val_loss2: -0.0140\n",
      "Epoch [3], val_loss1: 0.0425, val_loss2: -0.0212\n",
      "Epoch [4], val_loss1: 0.0429, val_loss2: -0.0257\n",
      "Epoch [5], val_loss1: 0.0434, val_loss2: -0.0289\n",
      "Epoch [6], val_loss1: 0.0439, val_loss2: -0.0313\n",
      "Epoch [7], val_loss1: 0.0445, val_loss2: -0.0333\n",
      "Epoch [8], val_loss1: 0.0451, val_loss2: -0.0350\n",
      "Epoch [9], val_loss1: 0.0457, val_loss2: -0.0365\n",
      "Epoch [10], val_loss1: 0.0463, val_loss2: -0.0378\n",
      "Epoch [11], val_loss1: 0.0470, val_loss2: -0.0391\n",
      "Epoch [12], val_loss1: 0.0478, val_loss2: -0.0404\n",
      "Epoch [13], val_loss1: 0.0488, val_loss2: -0.0418\n",
      "Epoch [14], val_loss1: 0.0501, val_loss2: -0.0434\n",
      "Epoch [15], val_loss1: 0.0516, val_loss2: -0.0452\n",
      "Epoch [16], val_loss1: 0.0532, val_loss2: -0.0471\n",
      "Epoch [17], val_loss1: 0.0546, val_loss2: -0.0487\n",
      "Epoch [18], val_loss1: 0.0550, val_loss2: -0.0494\n",
      "Epoch [19], val_loss1: 0.0547, val_loss2: -0.0493\n",
      "Epoch [20], val_loss1: 0.0538, val_loss2: -0.0488\n",
      "Epoch [21], val_loss1: 0.0530, val_loss2: -0.0482\n",
      "Epoch [22], val_loss1: 0.0523, val_loss2: -0.0478\n",
      "Epoch [23], val_loss1: 0.0521, val_loss2: -0.0478\n",
      "Epoch [24], val_loss1: 0.0523, val_loss2: -0.0482\n",
      "Epoch [25], val_loss1: 0.0531, val_loss2: -0.0491\n",
      "Epoch [26], val_loss1: 0.0543, val_loss2: -0.0505\n",
      "Epoch [27], val_loss1: 0.0561, val_loss2: -0.0524\n",
      "Epoch [28], val_loss1: 0.0587, val_loss2: -0.0550\n",
      "Epoch [29], val_loss1: 0.0620, val_loss2: -0.0583\n",
      "Epoch [30], val_loss1: 0.0655, val_loss2: -0.0618\n",
      "Epoch [31], val_loss1: 0.0677, val_loss2: -0.0640\n",
      "Epoch [32], val_loss1: 0.0671, val_loss2: -0.0635\n",
      "Epoch [33], val_loss1: 0.0643, val_loss2: -0.0609\n",
      "Epoch [34], val_loss1: 0.0610, val_loss2: -0.0577\n",
      "Epoch [35], val_loss1: 0.0579, val_loss2: -0.0549\n",
      "Epoch [36], val_loss1: 0.0554, val_loss2: -0.0526\n",
      "Epoch [37], val_loss1: 0.0536, val_loss2: -0.0509\n",
      "Epoch [38], val_loss1: 0.0523, val_loss2: -0.0497\n",
      "Epoch [39], val_loss1: 0.0515, val_loss2: -0.0490\n",
      "Epoch [40], val_loss1: 0.0510, val_loss2: -0.0486\n",
      "Epoch [41], val_loss1: 0.0507, val_loss2: -0.0484\n",
      "Epoch [42], val_loss1: 0.0505, val_loss2: -0.0483\n",
      "Epoch [43], val_loss1: 0.0505, val_loss2: -0.0483\n",
      "Epoch [44], val_loss1: 0.0506, val_loss2: -0.0484\n",
      "Epoch [45], val_loss1: 0.0508, val_loss2: -0.0487\n",
      "Epoch [46], val_loss1: 0.0512, val_loss2: -0.0492\n",
      "Epoch [47], val_loss1: 0.0518, val_loss2: -0.0497\n",
      "Epoch [48], val_loss1: 0.0524, val_loss2: -0.0503\n",
      "Epoch [49], val_loss1: 0.0530, val_loss2: -0.0509\n",
      "Epoch [50], val_loss1: 0.0536, val_loss2: -0.0515\n",
      "Epoch [51], val_loss1: 0.0541, val_loss2: -0.0521\n",
      "Epoch [52], val_loss1: 0.0546, val_loss2: -0.0527\n",
      "Epoch [53], val_loss1: 0.0552, val_loss2: -0.0533\n",
      "Epoch [54], val_loss1: 0.0559, val_loss2: -0.0540\n",
      "Epoch [55], val_loss1: 0.0567, val_loss2: -0.0548\n",
      "Epoch [56], val_loss1: 0.0576, val_loss2: -0.0557\n",
      "Epoch [57], val_loss1: 0.0586, val_loss2: -0.0568\n",
      "Epoch [58], val_loss1: 0.0596, val_loss2: -0.0578\n",
      "Epoch [59], val_loss1: 0.0606, val_loss2: -0.0588\n",
      "Epoch [60], val_loss1: 0.0616, val_loss2: -0.0598\n",
      "Epoch [61], val_loss1: 0.0624, val_loss2: -0.0607\n",
      "Epoch [62], val_loss1: 0.0632, val_loss2: -0.0615\n",
      "Epoch [63], val_loss1: 0.0639, val_loss2: -0.0622\n",
      "Epoch [64], val_loss1: 0.0644, val_loss2: -0.0627\n",
      "Epoch [65], val_loss1: 0.0646, val_loss2: -0.0630\n",
      "Epoch [66], val_loss1: 0.0646, val_loss2: -0.0630\n",
      "Epoch [67], val_loss1: 0.0643, val_loss2: -0.0627\n",
      "Epoch [68], val_loss1: 0.0637, val_loss2: -0.0621\n",
      "Epoch [69], val_loss1: 0.0630, val_loss2: -0.0615\n",
      "Epoch [70], val_loss1: 0.0622, val_loss2: -0.0607\n",
      "Epoch [71], val_loss1: 0.0615, val_loss2: -0.0600\n",
      "Epoch [72], val_loss1: 0.0607, val_loss2: -0.0592\n",
      "Epoch [73], val_loss1: 0.0600, val_loss2: -0.0586\n",
      "Epoch [74], val_loss1: 0.0595, val_loss2: -0.0581\n",
      "Epoch [75], val_loss1: 0.0590, val_loss2: -0.0576\n",
      "Epoch [76], val_loss1: 0.0586, val_loss2: -0.0572\n",
      "Epoch [77], val_loss1: 0.0582, val_loss2: -0.0569\n",
      "Epoch [78], val_loss1: 0.0581, val_loss2: -0.0568\n",
      "Epoch [79], val_loss1: 0.0581, val_loss2: -0.0568\n",
      "Epoch [80], val_loss1: 0.0581, val_loss2: -0.0568\n",
      "Epoch [81], val_loss1: 0.0581, val_loss2: -0.0568\n",
      "Epoch [82], val_loss1: 0.0580, val_loss2: -0.0567\n",
      "Epoch [83], val_loss1: 0.0580, val_loss2: -0.0567\n",
      "Epoch [84], val_loss1: 0.0582, val_loss2: -0.0569\n",
      "Epoch [85], val_loss1: 0.0585, val_loss2: -0.0572\n",
      "Epoch [86], val_loss1: 0.0588, val_loss2: -0.0576\n",
      "Epoch [87], val_loss1: 0.0593, val_loss2: -0.0581\n",
      "Epoch [88], val_loss1: 0.0598, val_loss2: -0.0586\n",
      "Epoch [89], val_loss1: 0.0602, val_loss2: -0.0590\n",
      "Epoch [90], val_loss1: 0.0607, val_loss2: -0.0595\n",
      "Epoch [91], val_loss1: 0.0611, val_loss2: -0.0599\n",
      "Epoch [92], val_loss1: 0.0614, val_loss2: -0.0603\n",
      "Epoch [93], val_loss1: 0.0618, val_loss2: -0.0606\n",
      "Epoch [94], val_loss1: 0.0620, val_loss2: -0.0609\n",
      "Epoch [95], val_loss1: 0.0623, val_loss2: -0.0611\n",
      "Epoch [96], val_loss1: 0.0624, val_loss2: -0.0613\n",
      "Epoch [97], val_loss1: 0.0625, val_loss2: -0.0614\n",
      "Epoch [98], val_loss1: 0.0624, val_loss2: -0.0613\n",
      "Epoch [99], val_loss1: 0.0618, val_loss2: -0.0607\n",
      "  anomaly detection...\n",
      "  Scores stats: min=0.045427, max=0.281028, mean=0.119704\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/usad_synthetic/synthetic.csv\n",
      "\n",
      "Results:\n",
      "  best_threshold: 0.1160\n",
      "  threshold: 0.11597149819135666\n",
      "  accuracy: 0.5850\n",
      "  precision: 0.3864\n",
      "  recall: 0.7327\n",
      "  f1: 0.5060\n",
      "  fnr: 0.2673\n",
      "  fpr: 0.4754\n",
      "  tp: 529\n",
      "  fp: 840\n",
      "  tn: 927\n",
      "  fn: 193\n"
     ]
    }
   ],
   "source": [
    "def process_synthetic_data_usad():\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Processing Synthetic Dataset (USAD)...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Paths\n",
    "    train_path = \"../../datasets/synthetic/train/train.csv\"\n",
    "    test_path = \"../../datasets/synthetic/test/test.csv\"\n",
    "    \n",
    "    if not os.path.exists(train_path):\n",
    "        print(f\"Error: {train_path} not found. Please run preprocess/synthetic.ipynb first.\")\n",
    "        return\n",
    "\n",
    "    # Read data\n",
    "    train_df_raw = pd.read_csv(train_path)\n",
    "    test_df_raw = pd.read_csv(test_path)\n",
    "    \n",
    "    print(f\"  Train shape: {train_df_raw.shape}\")\n",
    "    print(f\"  Test shape: {test_df_raw.shape}\")\n",
    "    \n",
    "    # Prepare Numeric Data\n",
    "    # Drop timestamp and label\n",
    "    cols_to_drop = [\"timestamp\", \"label\"]\n",
    "    train_vals = train_df_raw.drop([c for c in cols_to_drop if c in train_df_raw.columns], axis=1)\n",
    "    test_vals = test_df_raw.drop([c for c in cols_to_drop if c in test_df_raw.columns], axis=1)\n",
    "    \n",
    "    train_vals = train_vals.astype(float)\n",
    "    test_vals = test_vals.astype(float)\n",
    "    \n",
    "    # Normalization\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_train = min_max_scaler.fit_transform(train_vals.values)\n",
    "    x_test = min_max_scaler.transform(test_vals.values)\n",
    "    \n",
    "    # Windowing\n",
    "    def make_windows(data_arr):\n",
    "        n = data_arr.shape[0]\n",
    "        if n <= WINDOW_SIZE:\n",
    "            return np.empty((0, WINDOW_SIZE, data_arr.shape[1]))\n",
    "        indexer = np.arange(WINDOW_SIZE)[None, :] + np.arange(n - WINDOW_SIZE + 1)[:, None]\n",
    "        return data_arr[indexer]\n",
    "\n",
    "    train_windows = make_windows(x_train)\n",
    "    test_windows = make_windows(x_test)\n",
    "    \n",
    "    # Flatten for USAD\n",
    "    w_size = WINDOW_SIZE * x_train.shape[1]\n",
    "    z_size = WINDOW_SIZE * HIDDEN_SIZE\n",
    "    \n",
    "    train_windows_flat = train_windows.reshape(-1, w_size)\n",
    "    test_windows_flat = test_windows.reshape(-1, w_size)\n",
    "    \n",
    "    # DataLoaders\n",
    "    split_idx = int(0.8 * len(train_windows_flat))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        data_utils.TensorDataset(torch.from_numpy(train_windows_flat[:split_idx]).float()),\n",
    "        batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        data_utils.TensorDataset(torch.from_numpy(train_windows_flat[split_idx:]).float()),\n",
    "        batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        data_utils.TensorDataset(torch.from_numpy(test_windows_flat).float()),\n",
    "        batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model = USAD(N_EPOCHS, w_size, z_size).to(device())\n",
    "    \n",
    "    print(\"  training model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_loader, val_loader)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"  anomaly detection...\")\n",
    "    start_time = time.time()\n",
    "    results_list = model.predict(test_loader)\n",
    "    if len(results_list) > 0:\n",
    "        scores = torch.cat(results_list).cpu().numpy()\n",
    "    else:\n",
    "        scores = np.array([])\n",
    "    scoring_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Scores stats: min={scores.min():.6f}, max={scores.max():.6f}, mean={scores.mean():.6f}\")\n",
    "    \n",
    "    # Window-based Labeling Logic\n",
    "    if \"label\" in test_df_raw.columns:\n",
    "        test_true_labels = test_df_raw[\"label\"].to_numpy()\n",
    "        label_windows = make_windows(test_true_labels.reshape(-1, 1)).reshape(-1, WINDOW_SIZE)\n",
    "        y_test = (label_windows.sum(axis=1) > 0).astype(int)\n",
    "    else:\n",
    "        y_test = np.zeros(len(scores))\n",
    "\n",
    "    # Align lengths\n",
    "    min_len = min(len(scores), len(y_test))\n",
    "    scores = scores[:min_len]\n",
    "    y_test = y_test[:min_len]\n",
    "\n",
    "    print(\"  finding best threshold...\")\n",
    "    best_threshold, best_metrics = find_best_threshold(scores, y_test)\n",
    "    \n",
    "    # Save Results\n",
    "    results_dir_syn = \"../../results/models/usad_synthetic\"\n",
    "    os.makedirs(results_dir_syn, exist_ok=True)\n",
    "    \n",
    "    full_df = pd.concat([train_df_raw, test_df_raw])\n",
    "    \n",
    "    # Use first feature column for visualization\n",
    "    feature_cols = [c for c in train_df_raw.columns if c not in cols_to_drop]\n",
    "    if feature_cols:\n",
    "        complete_values = full_df[feature_cols[0]].to_numpy()\n",
    "    else:\n",
    "        complete_values = full_df.iloc[:, 0].to_numpy()\n",
    "\n",
    "    if \"label\" in full_df.columns:\n",
    "        complete_labels = full_df[\"label\"].to_numpy()\n",
    "    else:\n",
    "        complete_labels = np.zeros(len(complete_values))\n",
    "        \n",
    "    if \"timestamp\" in full_df.columns:\n",
    "        complete_timestamps = full_df[\"timestamp\"].to_numpy()\n",
    "    else:\n",
    "        complete_timestamps = range(len(complete_values))\n",
    "        \n",
    "    complete_predictions = np.zeros(len(complete_values))\n",
    "    complete_anomaly_scores = np.zeros(len(complete_values))\n",
    "    \n",
    "    # Align predictions\n",
    "    pred_start_idx = len(train_df_raw) + WINDOW_SIZE - 1\n",
    "    \n",
    "    end_idx = pred_start_idx + len(scores)\n",
    "    if end_idx > len(complete_values):\n",
    "        end_idx = len(complete_values)\n",
    "        scores = scores[:end_idx - pred_start_idx]\n",
    "        \n",
    "    complete_predictions[pred_start_idx:end_idx] = (scores > best_threshold).astype(int)\n",
    "    complete_anomaly_scores[pred_start_idx:end_idx] = scores\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "        'timestamp': complete_timestamps,\n",
    "        'value': complete_values,\n",
    "        'label': complete_labels,\n",
    "        'predicted': complete_predictions,\n",
    "        'anomaly_score': complete_anomaly_scores\n",
    "    })\n",
    "    \n",
    "    output_file = os.path.join(results_dir_syn, \"synthetic.csv\")\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"  results saved to: {output_file}\")\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  best_threshold: {best_threshold:.4f}\")\n",
    "    for k, v in best_metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "# Run\n",
    "process_synthetic_data_usad()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "USAD_test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "spectrum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
