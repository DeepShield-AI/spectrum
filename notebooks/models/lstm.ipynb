{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dabf357906194a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T02:12:47.540167228Z",
     "start_time": "2025-11-19T02:12:47.509840509Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from spectrum.config import WINDOW_SIZE\n",
    "from spectrum.utils import set_random_state\n",
    "from spectrum.models import LSTM\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.edgecolor\": \"0.3\",\n",
    "        \"axes.linewidth\": 0.8,\n",
    "        \"font.size\": 12,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"figure.dpi\": 120,\n",
    "        \"legend.frameon\": False,\n",
    "    }\n",
    ")\n",
    "set_random_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fcea11662cadf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T02:12:58.825995325Z",
     "start_time": "2025-11-19T02:12:47.541369213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 3 datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 1\n",
      "  training model...\n",
      "  anomaly detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:  33%|███▎      | 1/3 [00:55<01:51, 56.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores stats: min=0.000000, max=9.895761, mean=0.002547\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/lstm/1.csv\n",
      "  ID 1 completed:\n",
      "    best_threshold: 0.0000\n",
      "    f1: 0.8571\n",
      "    precision: 0.7500\n",
      "    recall: 1.0000\n",
      "    accuracy: 0.9993\n",
      "processing: 2\n",
      "  training model...\n",
      "  anomaly detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:  67%|██████▋   | 2/3 [01:52<00:56, 56.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores stats: min=0.000000, max=9.916924, mean=0.002551\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/lstm/2.csv\n",
      "  ID 2 completed:\n",
      "    best_threshold: 0.0000\n",
      "    f1: 0.8571\n",
      "    precision: 0.7500\n",
      "    recall: 1.0000\n",
      "    accuracy: 0.9993\n",
      "processing: 3\n",
      "  training model...\n",
      "  anomaly detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 3/3 [03:52<00:00, 77.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores stats: min=0.000000, max=9.916239, mean=0.002540\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/lstm/3.csv\n",
      "  ID 3 completed:\n",
      "    best_threshold: 5.7986\n",
      "    f1: 0.0000\n",
      "    precision: 0.0000\n",
      "    recall: 0.0000\n",
      "    accuracy: 0.9998\n",
      "summary results saved to: ../../results/models/lstm/lstm.csv\n",
      "\n",
      "================================================================================\n",
      "LSTM anomaly detection results\n",
      "================================================================================\n",
      "processed 3 datasets\n",
      "average F1: 0.5714 ± 0.4949\n",
      "average precision: 0.5000 ± 0.4330\n",
      "average recall: 0.6667 ± 0.5774\n",
      "average accuracy: 0.9995 ± 0.0003\n",
      "average training time: 77.24s\n",
      "average scoring time: 0.30s\n",
      "================================================================================\n",
      "details:\n",
      "   id      f1  precision  recall  accuracy  best_threshold  tp  fp    tn  fn\n",
      "0   1  0.8571       0.75     1.0    0.9993          0.0000   9   3  4292   0\n",
      "1   2  0.8571       0.75     1.0    0.9993          0.0000   9   3  4292   0\n",
      "2   3  0.0000       0.00     0.0    0.9998          5.7986   0   1  4303   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "selected_ids = [1, 2, 3]\n",
    "\n",
    "results_dir = \"../../results/models/lstm\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def find_best_threshold(scores, true_labels, thresholds=None):\n",
    "    # Safety check for NaN/Inf in scores\n",
    "    if np.isnan(scores).any() or np.isinf(scores).any():\n",
    "        print(\"Warning: Scores contain NaN or Inf. Handling them.\")\n",
    "        scores = np.nan_to_num(scores, nan=0.0, posinf=np.max(scores[np.isfinite(scores)]) if np.any(np.isfinite(scores)) else 1.0)\n",
    "\n",
    "    if thresholds is None:\n",
    "        # Handle case where scores are all same (e.g. 0)\n",
    "        if np.min(scores) == np.max(scores):\n",
    "            thresholds = [scores[0]]\n",
    "        else:\n",
    "            # use percentiles as candidate thresholds\n",
    "            thresholds = [np.percentile(scores, p) for p in range(0, 90, 5)]\n",
    "            thresholds.extend([np.percentile(scores, p) for p in range(90, 100, 1)])\n",
    "            thresholds.extend([np.percentile(scores, p) for p in [99.1, 99.3, 99.5, 99.7, 99.9, 99.95, 99.99]])\n",
    "    \n",
    "    # Remove duplicates and sort descending\n",
    "    thresholds = sorted(list(set(thresholds)), reverse=True)\n",
    "\n",
    "    best_f1 = -1 \n",
    "    best_threshold = thresholds[0] if len(thresholds) > 0 else 0.0\n",
    "    \n",
    "    best_metrics = {\n",
    "        'threshold': best_threshold,\n",
    "        'accuracy': 0.0,\n",
    "        'precision': 0.0,\n",
    "        'recall': 0.0,\n",
    "        'f1': 0.0,\n",
    "        'fnr': 0.0,\n",
    "        'fpr': 0.0,\n",
    "        'tp': 0,\n",
    "        'fp': 0,\n",
    "        'tn': 0,\n",
    "        'fn': 0\n",
    "    }\n",
    "\n",
    "    # Ensure true_labels is int\n",
    "    true_labels = true_labels.astype(int)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (scores > threshold).astype(int)\n",
    "\n",
    "        # calculate confusion matrix\n",
    "        TP = ((true_labels == 1) & (pred_labels == 1)).sum()\n",
    "        FP = ((true_labels == 0) & (pred_labels == 1)).sum()\n",
    "        TN = ((true_labels == 0) & (pred_labels == 0)).sum()\n",
    "        FN = ((true_labels == 1) & (pred_labels == 0)).sum()\n",
    "\n",
    "        # calculate metrics\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                'threshold': threshold,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'fnr': fnr,\n",
    "                'fpr': fpr,\n",
    "                'tp': TP,\n",
    "                'fp': FP,\n",
    "                'tn': TN,\n",
    "                'fn': FN\n",
    "            }\n",
    "            \n",
    "    return best_threshold, best_metrics\n",
    "\n",
    "\n",
    "def process_single_id(data_id):\n",
    "    print(f\"processing: {data_id}\")\n",
    "\n",
    "    train_data = pl.read_csv(f\"../../datasets/Tencent/train/{data_id}.csv\")\n",
    "    test_data = pl.read_csv(f\"../../datasets/Tencent/test/{data_id}.csv\")\n",
    "\n",
    "    print(\"  training model...\")\n",
    "    # output_dims = [1, 2, 3, 4, 5] (features are cols 1-5, col 0 is timestamp)\n",
    "    model = LSTM(input_size=5, output_dims=list(range(1, 6)), epochs=100)\n",
    "    start_time = time.time()\n",
    "    model.fit(train_data.to_numpy())\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    print(\"  anomaly detection...\")\n",
    "    start_time = time.time()\n",
    "    scores = model.predict(test_data.to_numpy())\n",
    "    scoring_time = time.time() - start_time\n",
    "    \n",
    "    # Debug stats\n",
    "    print(f\"  Scores stats: min={scores.min():.6f}, max={scores.max():.6f}, mean={scores.mean():.6f}\")\n",
    "\n",
    "    scores_array = scores\n",
    "\n",
    "    test_true_labels = test_data[\"label\"].to_numpy()\n",
    "    \n",
    "    # Align labels with scores\n",
    "    labels_start_idx = len(test_true_labels) - len(scores_array)\n",
    "    aligned_labels = test_true_labels[labels_start_idx:]\n",
    "\n",
    "    print(\"  finding best threshold...\")\n",
    "    best_threshold, best_metrics = find_best_threshold(scores_array, aligned_labels)\n",
    "\n",
    "    # --- Combine Train and Test for \"Complete\" data ---\n",
    "    full_df = pl.concat([train_data, test_data])\n",
    "\n",
    "    if \"value\" in full_df.columns:\n",
    "        complete_values = full_df[\"value\"].to_numpy()\n",
    "    else:\n",
    "        # Use column 1 (first feature) as representative value\n",
    "        complete_values = full_df.to_numpy()[:, 1]\n",
    "\n",
    "    complete_labels = full_df[\"label\"].to_numpy()\n",
    "    \n",
    "    if \"timestamp\" in full_df.columns:\n",
    "        complete_timestamps = full_df[\"timestamp\"].to_numpy()\n",
    "    else:\n",
    "        complete_timestamps = range(len(complete_values))\n",
    "\n",
    "    complete_predictions = np.zeros(len(complete_values))\n",
    "    complete_anomaly_scores = np.zeros(len(complete_values))\n",
    "\n",
    "    # Align predictions with the full data\n",
    "    pred_start_idx = len(train_data) + (len(test_data) - len(scores_array))\n",
    "    \n",
    "    complete_predictions[pred_start_idx:] = (scores_array > best_threshold).astype(int)\n",
    "    complete_anomaly_scores[pred_start_idx:] = scores_array\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'timestamp': complete_timestamps,\n",
    "        'value': complete_values,\n",
    "        'label': complete_labels,\n",
    "        'predicted': complete_predictions,\n",
    "        'anomaly_score': complete_anomaly_scores\n",
    "    })\n",
    "\n",
    "    output_file = os.path.join(results_dir, f\"{data_id}.csv\")\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"  results saved to: {output_file}\")\n",
    "\n",
    "    return {\n",
    "        'id': data_id,\n",
    "        'training_time': training_time,\n",
    "        'testing_time': scoring_time,\n",
    "        'total_time': training_time + scoring_time,\n",
    "        'train_samples': len(train_data),\n",
    "        'test_samples': len(test_data),\n",
    "        'best_threshold': best_threshold,\n",
    "        **best_metrics\n",
    "    }\n",
    "\n",
    "\n",
    "all_results = []\n",
    "print(f\"processing {len(selected_ids)} datasets...\")\n",
    "\n",
    "for data_id in tqdm(selected_ids, desc=\"processing\"):\n",
    "    try:\n",
    "        result = process_single_id(data_id)\n",
    "        all_results.append(result)\n",
    "\n",
    "        print(f\"  ID {data_id} completed:\")\n",
    "        print(f\"    best_threshold: {result['best_threshold']:.4f}\")\n",
    "        print(f\"    f1: {result['f1']:.4f}\")\n",
    "        print(f\"    precision: {result['precision']:.4f}\")\n",
    "        print(f\"    recall: {result['recall']:.4f}\")\n",
    "        print(f\"    accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  processing {data_id} failed: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "if all_results:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    summary_file = os.path.join(results_dir, \"lstm.csv\")\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"summary results saved to: {summary_file}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LSTM anomaly detection results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"processed {len(all_results)} datasets\")\n",
    "    print(f\"average F1: {summary_df['f1'].mean():.4f} ± {summary_df['f1'].std():.4f}\")\n",
    "    print(f\"average precision: {summary_df['precision'].mean():.4f} ± {summary_df['precision'].std():.4f}\")\n",
    "    print(f\"average recall: {summary_df['recall'].mean():.4f} ± {summary_df['recall'].std():.4f}\")\n",
    "    print(f\"average accuracy: {summary_df['accuracy'].mean():.4f} ± {summary_df['accuracy'].std():.4f}\")\n",
    "    print(f\"average training time: {summary_df['training_time'].mean():.2f}s\")\n",
    "    print(f\"average scoring time: {summary_df['testing_time'].mean():.2f}s\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"details:\")\n",
    "    display_cols = ['id', 'f1', 'precision', 'recall', 'accuracy', 'best_threshold', 'tp', 'fp', 'tn', 'fn']\n",
    "    print(summary_df[display_cols].round(4))\n",
    "else:\n",
    "    print(\"no results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3de9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing Synthetic Dataset...\n",
      "========================================\n",
      "  Train shape: (2500, 12)\n",
      "  Test shape: (2500, 12)\n",
      "  training model...\n",
      "  Feature columns: 10\n",
      "  anomaly detection...\n",
      "  Scores stats: min=0.000000, max=3.955362, mean=0.214955\n",
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/lstm_synthetic/synthetic.csv\n",
      "\n",
      "Results:\n",
      "  best_threshold: 0.9562\n",
      "  threshold: 0.9561876654624939\n",
      "  accuracy: 0.9722\n",
      "  precision: 0.6933\n",
      "  recall: 0.5306\n",
      "  f1: 0.6012\n",
      "  fnr: 0.4694\n",
      "  fpr: 0.0096\n",
      "  tp: 52\n",
      "  fp: 23\n",
      "  tn: 2363\n",
      "  fn: 46\n"
     ]
    }
   ],
   "source": [
    "def process_synthetic_data():\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Processing Synthetic Dataset...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Paths\n",
    "    train_path = \"../../datasets/synthetic/train/train.csv\"\n",
    "    test_path = \"../../datasets/synthetic/test/test.csv\"\n",
    "    \n",
    "    if not os.path.exists(train_path):\n",
    "        print(f\"Error: {train_path} not found. Please run preprocess/synthetic.ipynb first.\")\n",
    "        return\n",
    "\n",
    "    train_data = pl.read_csv(train_path)\n",
    "    test_data = pl.read_csv(test_path)\n",
    "\n",
    "    print(f\"  Train shape: {train_data.shape}\")\n",
    "    print(f\"  Test shape: {test_data.shape}\")\n",
    "\n",
    "    print(\"  training model...\")\n",
    "    \n",
    "    # Prepare numeric data for model\n",
    "    # Drop timestamp and label\n",
    "    cols_to_drop = [\"timestamp\", \"label\"]\n",
    "    # Select only feature columns\n",
    "    # Filter columns that are NOT in cols_to_drop\n",
    "    feature_cols = [c for c in train_data.columns if c not in cols_to_drop]\n",
    "    \n",
    "    train_features = train_data.select(feature_cols)\n",
    "    test_features = test_data.select(feature_cols)\n",
    "    \n",
    "    print(f\"  Feature columns: {len(feature_cols)}\")\n",
    "    \n",
    "    input_size = len(feature_cols)\n",
    "    output_dims = list(range(input_size)) # All features 0..N-1\n",
    "    \n",
    "    model = LSTM(input_size=input_size, output_dims=output_dims, epochs=100)\n",
    "    start_time = time.time()\n",
    "    model.fit(train_features.to_numpy())\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    print(\"  anomaly detection...\")\n",
    "    start_time = time.time()\n",
    "    scores = model.predict(test_features.to_numpy())\n",
    "    scoring_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Scores stats: min={scores.min():.6f}, max={scores.max():.6f}, mean={scores.mean():.6f}\")\n",
    "\n",
    "    scores_array = scores\n",
    "    test_true_labels = test_data[\"label\"].to_numpy()\n",
    "    \n",
    "    # Align labels\n",
    "    labels_start_idx = len(test_true_labels) - len(scores_array)\n",
    "    aligned_labels = test_true_labels[labels_start_idx:]\n",
    "\n",
    "    print(\"  finding best threshold...\")\n",
    "    best_threshold, best_metrics = find_best_threshold(scores_array, aligned_labels)\n",
    "    \n",
    "    # Save Results\n",
    "    results_dir_syn = \"../../results/models/lstm_synthetic\"\n",
    "    os.makedirs(results_dir_syn, exist_ok=True)\n",
    "    \n",
    "    full_df = pl.concat([train_data, test_data])\n",
    "    \n",
    "    # Use first feature (column 1, or feature_cols[0]) for visualization\n",
    "    # full_df columns: timestamp, features..., label\n",
    "    # We want the first feature value\n",
    "    first_feat_col = feature_cols[0]\n",
    "    complete_values = full_df[first_feat_col].to_numpy()\n",
    "    \n",
    "    complete_labels = full_df[\"label\"].to_numpy()\n",
    "    if \"timestamp\" in full_df.columns:\n",
    "        complete_timestamps = full_df[\"timestamp\"].to_numpy()\n",
    "    else:\n",
    "        complete_timestamps = range(len(complete_values))\n",
    "        \n",
    "    complete_predictions = np.zeros(len(complete_values))\n",
    "    complete_anomaly_scores = np.zeros(len(complete_values))\n",
    "    \n",
    "    pred_start_idx = len(train_data) + (len(test_data) - len(scores_array))\n",
    "    \n",
    "    # Ensure bounds\n",
    "    end_idx = pred_start_idx + len(scores_array)\n",
    "    complete_predictions[pred_start_idx:end_idx] = (scores_array > best_threshold).astype(int)\n",
    "    complete_anomaly_scores[pred_start_idx:end_idx] = scores_array\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "        'timestamp': complete_timestamps,\n",
    "        'value': complete_values,\n",
    "        'label': complete_labels,\n",
    "        'predicted': complete_predictions,\n",
    "        'anomaly_score': complete_anomaly_scores\n",
    "    })\n",
    "    \n",
    "    output_file = os.path.join(results_dir_syn, \"synthetic.csv\")\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"  results saved to: {output_file}\")\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  best_threshold: {best_threshold:.4f}\")\n",
    "    for k, v in best_metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "# Run\n",
    "process_synthetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea7c47",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
