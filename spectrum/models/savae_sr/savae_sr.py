import numpy as np
import torch
import torch.distributions as dist
import torch.nn.functional as F
from scipy.special import erf
from torch.nn.utils import clip_grad_norm_
from torch.optim import SGD
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from .source import VAE
from .source.loss import AE, KL0, KL
from .source.missing_value_imputation import mcmc_missing_imputation
from .source.utils import TestLoop
from ...config import WINDOW_SIZE
from ...utils import device


class SaVAE_SR:
    def __init__(
            self,
            window_size: int = WINDOW_SIZE,
            epochs: int = 60,
            batch_size: int = 256,
            latent_size: int = 3,
            positive_margin: int = 15
    ):
        self.epochs = epochs
        self.window_size = window_size
        self.batch_size = batch_size
        self.latent_size = latent_size
        self.margin = positive_margin

        self.alpha = torch.tensor(1.0).to(device())
        self.beta = torch.tensor(1.0).to(device())
        self.margin = torch.tensor(positive_margin).to(device())
        self.z_prior_dist = dist.Normal(
            torch.from_numpy(np.zeros((latent_size,), np.float32)).to(device()),
            torch.from_numpy(np.ones((latent_size,), np.float32)).to(device()))
        self.x_prior_dist = dist.Normal(
            torch.from_numpy(np.zeros((window_size,), np.float32)).to(device()),
            torch.from_numpy(np.ones((window_size,), np.float32)).to(device()))
        self.model = VAE(x_dim=self.window_size, z_dim=self.latent_size).to(device())

    def fit(self, dataset: Dataset):
        self.model.train()
        train_loader = DataLoader(dataset=dataset, shuffle=True, batch_size=self.batch_size, drop_last=True)
        # optimizer
        optimizerD = SGD(self.model.encoder.parameters(), lr=0.0002)
        optimizerG = SGD(self.model.generator.parameters(), lr=0.0005)
        lr_scheduler_D = StepLR(optimizerD, step_size=10, gamma=0.75)
        lr_scheduler_G = StepLR(optimizerG, step_size=10, gamma=0.75)
        # 1.has zp and zr modules
        for epoch in range(1, self.epochs + 1):
            loop = tqdm(enumerate(train_loader), total=len(train_loader),
                        desc=f"Epoch {epoch}/{self.epochs}", leave=True)
            for idx, (observe_x, observe_normal) in loop:
                optimizerD.zero_grad()
                optimizerG.zero_grad()
                # |--------------------Update Encoder--------------------|
                p_x_z, p_z_x, observe_z = self.model(observe_x)

                # Fake sample generated by the noise zp
                zp = self.z_prior_dist.sample((self.batch_size,))
                xp_mean, xp_std = self.model.generator(zp)
                xp = self.reparameterization(xp_mean, xp_std)

                # Fake sample generated by the noise z
                xr_mean, xr_std = self.model.generator(observe_z.detach())
                xr = self.reparameterization(xr_mean, xr_std)

                zp_mean, zp_std = self.model.encoder(xp.detach())
                p_zp_xp = dist.Normal(zp_mean, zp_std)
                observe_zp = p_zp_xp.sample()

                zr_mean, zr_std = self.model.encoder(xr.detach())
                p_zr_xr = dist.Normal(zr_mean, zr_std)
                observe_zr = p_zr_xr.sample()

                # Compute the loss of encoder
                kl0 = KL(observe_z, observe_normal, p_z_x, self.z_prior_dist)
                kl1 = KL0(observe_zr, observe_normal, p_zr_xr, self.z_prior_dist)
                kl2 = KL0(observe_zp, observe_normal, p_zp_xp, self.z_prior_dist)

                L1 = F.relu(self.margin - kl1)
                L2 = F.relu(self.margin - kl2)
                rec_loss = AE(observe_x, observe_normal, p_x_z)

                DLoss = self.alpha * (L1 + L2) + self.beta * rec_loss + kl0 + self.model.encoder.penalty() * 0.001
                DLoss.backward(retain_graph=True)
                clip_grad_norm_(self.model.encoder.parameters(), max_norm=10.)
                optimizerD.step()

                # |--------------------Update Generator--------------------|
                zr_mean, zr_std = self.model.encoder(xr)
                zp_mean, zp_std = self.model.encoder(xp)
                p_zr_xr = dist.Normal(zr_mean, zr_std)
                p_zp_xp = dist.Normal(zp_mean, zp_std)

                observe_zr = p_zr_xr.sample((self.batch_size,))
                observe_zp = p_zp_xp.sample((self.batch_size,))

                kl1 = KL0(observe_zr, observe_normal, p_zr_xr, self.z_prior_dist)
                kl2 = KL0(observe_zp, observe_normal, p_zp_xp, self.z_prior_dist)

                GLoss = self.alpha * (kl1 + kl2) + self.model.generator.penalty() * 0.001

                GLoss.backward()
                clip_grad_norm_(self.model.generator.parameters(), max_norm=10.)
                optimizerG.step()

                loss = (GLoss + DLoss).item()
                loop.set_postfix(loss=loss / (idx + 1))
            lr_scheduler_D.step()
            lr_scheduler_G.step()

    def predict(self, dataset: Dataset, indicator_name="indicator"):
        """
        :param dataset: Time series values (polars Series)
        :param indicator_name:
            default "indicator": Reconstructed probability
            "indicator_prior": E_q(z|x)[log p(x|z) * p(z) / q(z|x)]
            "indicator_erf": erf(abs(x - x_mean) / x_std * scale_factor)
        :return:
        """
        with torch.no_grad():
            with TestLoop(use_cuda=False, print_fn=print).with_context() as loop:
                test_dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=False,
                                             drop_last=False)
                self.model.eval()

                for _, batch_data in loop.iter_steps(test_dataloader):
                    observe_x, observe_normal = batch_data  # type: Variable

                    observe_x = mcmc_missing_imputation(observe_normal=observe_normal,
                                                        vae=self.model,
                                                        n_iteration=10,
                                                        x=observe_x)
                    p_x_z, p_z_x, observe_z = self.model(observe_x)
                    # Fake sample generated by the noise zp
                    zp = self.z_prior_dist.sample((self.batch_size,))
                    xp_mean, xp_std = self.model.generator(zp)
                    xp = self.reparameterization(xp_mean, xp_std)
                    # Fake sample generated by the noise z
                    xr = p_x_z.sample()

                    zp_mean, zp_std = self.model.encoder(xp)
                    p_zp_xp = dist.Normal(zp_mean, zp_std)
                    observe_zp = p_zp_xp.sample()

                    zr_mean, zr_std = self.model.encoder(xr)
                    p_zr_xr = dist.Normal(zr_mean, zr_std)
                    observe_zr = p_zr_xr.sample()

                    # Compute the loss of encoder
                    kl0 = KL0(observe_z, observe_normal, p_z_x, self.z_prior_dist)
                    kl1 = KL0(observe_zr, observe_normal, p_zr_xr, self.z_prior_dist)
                    kl2 = KL0(observe_zp, observe_normal, p_zp_xp, self.z_prior_dist)

                    L1 = F.relu(self.margin - kl1)
                    L2 = F.relu(self.margin - kl2)
                    rec_loss = AE(observe_x, observe_normal, p_x_z)

                    DLoss = self.alpha * (L1 + L2) + self.beta * rec_loss + kl0 + self.model.encoder.penalty() * 0.001
                    GLoss = self.alpha * (kl1 + kl2) + self.beta * rec_loss + self.model.generator.penalty() * 0.001

                    loss = (GLoss + DLoss).item()

                    loop.submit_metric("test_loss", loss)
                    log_p_xz = p_x_z.log_prob(observe_x).data.cpu().numpy()

                    log_p_x = log_p_xz * np.sum(
                        torch.exp(self.z_prior_dist.log_prob(observe_z) - p_z_x.log_prob(observe_z)).cpu().numpy(),
                        axis=-1, keepdims=True)

                    indicator_erf = erf((torch.abs(observe_x - p_x_z.mean) / p_x_z.stddev).cpu().numpy() * 0.1589967)

                    loop.submit_data("indicator", -np.mean(log_p_xz[:, :, -1], axis=0))
                    loop.submit_data("indicator_prior", -np.mean(log_p_x[:, :, -1], axis=0))
                    loop.submit_data("indicator_erf", np.mean(indicator_erf[:, :, -1], axis=0))

                indicator = np.concatenate(loop.get_data_by_name(indicator_name))

            indicator = np.concatenate([np.ones(shape=self.window_size - 1) * np.min(indicator), indicator])
            return indicator

    def reparameterization(self, mu, sd, latent=False):
        if latent:
            noise = self.z_prior_dist.sample((self.batch_size,))
        else:
            noise = self.x_prior_dist.sample((self.batch_size,))
        return noise * sd + mu

    # # Testing
    # # compute the precision, recall and best F1-score of testing set
    # y_prob_train = model.predict(test_kpi.label_sampling(0.))
    # y_prob_train = range_lift_with_delay(y_prob_train, test_kpi.truth, delay=7)
    # precisions, recalls, thresholds = metrics.precision_recall_curve(test_kpi.truth, y_prob_train)
    # f1_scores = (2 * precisions * recalls) / (precisions + recalls)
    # ind = np.argmax(f1_scores[np.isfinite(f1_scores)])
    # precision = precisions[np.isfinite(precisions)][ind]
    # recall = recalls[np.isfinite(recalls)][ind]
    # best_f1 = np.max(f1_scores[np.isfinite(f1_scores)])
