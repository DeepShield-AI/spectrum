{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86ffa4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T03:06:06.633976646Z",
     "start_time": "2025-11-19T03:06:06.624471108Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from spectrum.utils.random import set_random_state\n",
    "from spectrum.models import SRCNN\n",
    "from spectrum.dataset import SRYaHooDataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.edgecolor\": \"0.3\",\n",
    "        \"axes.linewidth\": 0.8,\n",
    "        \"font.size\": 12,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"figure.dpi\": 120,\n",
    "        \"legend.frameon\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "set_random_state(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4990c97f153ffc9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T03:06:11.057137243Z",
     "start_time": "2025-11-19T03:06:06.634865152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 3 datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 4\n",
      "  training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: 0it [00:00, ?it/s]\n",
      "Epoch 2/60: 0it [00:00, ?it/s]\n",
      "Epoch 3/60: 0it [00:00, ?it/s]\n",
      "Epoch 4/60: 0it [00:00, ?it/s]\n",
      "Epoch 5/60: 0it [00:00, ?it/s]\n",
      "Epoch 6/60: 0it [00:00, ?it/s]\n",
      "Epoch 7/60: 0it [00:00, ?it/s]\n",
      "Epoch 8/60: 0it [00:00, ?it/s]\n",
      "Epoch 9/60: 0it [00:00, ?it/s]\n",
      "Epoch 10/60: 0it [00:00, ?it/s]\n",
      "Epoch 11/60: 0it [00:00, ?it/s]\n",
      "Epoch 12/60: 0it [00:00, ?it/s]\n",
      "Epoch 13/60: 0it [00:00, ?it/s]\n",
      "Epoch 14/60: 0it [00:00, ?it/s]\n",
      "Epoch 15/60: 0it [00:00, ?it/s]\n",
      "Epoch 16/60: 0it [00:00, ?it/s]\n",
      "Epoch 17/60: 0it [00:00, ?it/s]\n",
      "Epoch 18/60: 0it [00:00, ?it/s]\n",
      "Epoch 19/60: 0it [00:00, ?it/s]\n",
      "Epoch 20/60: 0it [00:00, ?it/s]\n",
      "Epoch 21/60: 0it [00:00, ?it/s]\n",
      "Epoch 22/60: 0it [00:00, ?it/s]\n",
      "Epoch 23/60: 0it [00:00, ?it/s]\n",
      "Epoch 24/60: 0it [00:00, ?it/s]\n",
      "Epoch 25/60: 0it [00:00, ?it/s]\n",
      "Epoch 26/60: 0it [00:00, ?it/s]\n",
      "Epoch 27/60: 0it [00:00, ?it/s]\n",
      "Epoch 28/60: 0it [00:00, ?it/s]\n",
      "Epoch 29/60: 0it [00:00, ?it/s]\n",
      "Epoch 30/60: 0it [00:00, ?it/s]\n",
      "Epoch 31/60: 0it [00:00, ?it/s]\n",
      "Epoch 32/60: 0it [00:00, ?it/s]\n",
      "Epoch 33/60: 0it [00:00, ?it/s]\n",
      "Epoch 34/60: 0it [00:00, ?it/s]\n",
      "Epoch 35/60: 0it [00:00, ?it/s]\n",
      "Epoch 36/60: 0it [00:00, ?it/s]\n",
      "Epoch 37/60: 0it [00:00, ?it/s]\n",
      "Epoch 38/60: 0it [00:00, ?it/s]\n",
      "Epoch 39/60: 0it [00:00, ?it/s]\n",
      "Epoch 40/60: 0it [00:00, ?it/s]\n",
      "Epoch 41/60: 0it [00:00, ?it/s]\n",
      "Epoch 42/60: 0it [00:00, ?it/s]\n",
      "Epoch 43/60: 0it [00:00, ?it/s]\n",
      "Epoch 44/60: 0it [00:00, ?it/s]\n",
      "Epoch 45/60: 0it [00:00, ?it/s]\n",
      "Epoch 46/60: 0it [00:00, ?it/s]\n",
      "Epoch 47/60: 0it [00:00, ?it/s]\n",
      "Epoch 48/60: 0it [00:00, ?it/s]\n",
      "Epoch 49/60: 0it [00:00, ?it/s]\n",
      "Epoch 50/60: 0it [00:00, ?it/s]\n",
      "Epoch 51/60: 0it [00:00, ?it/s]\n",
      "Epoch 52/60: 0it [00:00, ?it/s]\n",
      "Epoch 53/60: 0it [00:00, ?it/s]\n",
      "Epoch 54/60: 0it [00:00, ?it/s]\n",
      "Epoch 55/60: 0it [00:00, ?it/s]\n",
      "Epoch 56/60: 0it [00:00, ?it/s]\n",
      "Epoch 57/60: 0it [00:00, ?it/s]\n",
      "Epoch 58/60: 0it [00:00, ?it/s]\n",
      "Epoch 59/60: 0it [00:00, ?it/s]\n",
      "Epoch 60/60: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.16 seconds\n",
      "  anomaly detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing:  33%|███▎      | 1/3 [00:01<00:03,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/sr_cnn/4.csv\n",
      "  ID 4 completed:\n",
      "    best_threshold: 0.6093\n",
      "    f1: 0.1579\n",
      "    precision: 0.1034\n",
      "    recall: 0.3333\n",
      "    accuracy: 0.9551\n",
      "processing: 17\n",
      "  training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: 0it [00:00, ?it/s]\n",
      "Epoch 2/60: 0it [00:00, ?it/s]\n",
      "Epoch 3/60: 0it [00:00, ?it/s]\n",
      "Epoch 4/60: 0it [00:00, ?it/s]\n",
      "Epoch 5/60: 0it [00:00, ?it/s]\n",
      "Epoch 6/60: 0it [00:00, ?it/s]\n",
      "Epoch 7/60: 0it [00:00, ?it/s]\n",
      "Epoch 8/60: 0it [00:00, ?it/s]\n",
      "Epoch 9/60: 0it [00:00, ?it/s]\n",
      "Epoch 10/60: 0it [00:00, ?it/s]\n",
      "Epoch 11/60: 0it [00:00, ?it/s]\n",
      "Epoch 12/60: 0it [00:00, ?it/s]\n",
      "Epoch 13/60: 0it [00:00, ?it/s]\n",
      "Epoch 14/60: 0it [00:00, ?it/s]\n",
      "Epoch 15/60: 0it [00:00, ?it/s]\n",
      "Epoch 16/60: 0it [00:00, ?it/s]\n",
      "Epoch 17/60: 0it [00:00, ?it/s]\n",
      "Epoch 18/60: 0it [00:00, ?it/s]\n",
      "Epoch 19/60: 0it [00:00, ?it/s]\n",
      "Epoch 20/60: 0it [00:00, ?it/s]\n",
      "Epoch 21/60: 0it [00:00, ?it/s]\n",
      "Epoch 22/60: 0it [00:00, ?it/s]\n",
      "Epoch 23/60: 0it [00:00, ?it/s]\n",
      "Epoch 24/60: 0it [00:00, ?it/s]\n",
      "Epoch 25/60: 0it [00:00, ?it/s]\n",
      "Epoch 26/60: 0it [00:00, ?it/s]\n",
      "Epoch 27/60: 0it [00:00, ?it/s]\n",
      "Epoch 28/60: 0it [00:00, ?it/s]\n",
      "Epoch 29/60: 0it [00:00, ?it/s]\n",
      "Epoch 30/60: 0it [00:00, ?it/s]\n",
      "Epoch 31/60: 0it [00:00, ?it/s]\n",
      "Epoch 32/60: 0it [00:00, ?it/s]\n",
      "Epoch 33/60: 0it [00:00, ?it/s]\n",
      "Epoch 34/60: 0it [00:00, ?it/s]\n",
      "Epoch 35/60: 0it [00:00, ?it/s]\n",
      "Epoch 36/60: 0it [00:00, ?it/s]\n",
      "Epoch 37/60: 0it [00:00, ?it/s]\n",
      "Epoch 38/60: 0it [00:00, ?it/s]\n",
      "Epoch 39/60: 0it [00:00, ?it/s]\n",
      "Epoch 40/60: 0it [00:00, ?it/s]\n",
      "Epoch 41/60: 0it [00:00, ?it/s]\n",
      "Epoch 42/60: 0it [00:00, ?it/s]\n",
      "Epoch 43/60: 0it [00:00, ?it/s]\n",
      "Epoch 44/60: 0it [00:00, ?it/s]\n",
      "Epoch 45/60: 0it [00:00, ?it/s]\n",
      "Epoch 46/60: 0it [00:00, ?it/s]\n",
      "Epoch 47/60: 0it [00:00, ?it/s]\n",
      "Epoch 48/60: 0it [00:00, ?it/s]\n",
      "Epoch 49/60: 0it [00:00, ?it/s]\n",
      "Epoch 50/60: 0it [00:00, ?it/s]\n",
      "Epoch 51/60: 0it [00:00, ?it/s]\n",
      "Epoch 52/60: 0it [00:00, ?it/s]\n",
      "Epoch 53/60: 0it [00:00, ?it/s]\n",
      "Epoch 54/60: 0it [00:00, ?it/s]\n",
      "Epoch 55/60: 0it [00:00, ?it/s]\n",
      "Epoch 56/60: 0it [00:00, ?it/s]\n",
      "Epoch 57/60: 0it [00:00, ?it/s]\n",
      "Epoch 58/60: 0it [00:00, ?it/s]\n",
      "Epoch 59/60: 0it [00:00, ?it/s]\n",
      "Epoch 60/60: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.15 seconds\n",
      "  anomaly detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "processing:  67%|██████▋   | 2/3 [00:02<00:01,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/sr_cnn/17.csv\n",
      "  ID 17 completed:\n",
      "    best_threshold: 0.4488\n",
      "    f1: 0.2590\n",
      "    precision: 0.1826\n",
      "    recall: 0.4452\n",
      "    accuracy: 0.4775\n",
      "processing: 33\n",
      "  training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: 0it [00:00, ?it/s]\n",
      "Epoch 2/60: 0it [00:00, ?it/s]\n",
      "Epoch 3/60: 0it [00:00, ?it/s]\n",
      "Epoch 4/60: 0it [00:00, ?it/s]\n",
      "Epoch 5/60: 0it [00:00, ?it/s]\n",
      "Epoch 6/60: 0it [00:00, ?it/s]\n",
      "Epoch 7/60: 0it [00:00, ?it/s]\n",
      "Epoch 8/60: 0it [00:00, ?it/s]\n",
      "Epoch 9/60: 0it [00:00, ?it/s]\n",
      "Epoch 10/60: 0it [00:00, ?it/s]\n",
      "Epoch 11/60: 0it [00:00, ?it/s]\n",
      "Epoch 12/60: 0it [00:00, ?it/s]\n",
      "Epoch 13/60: 0it [00:00, ?it/s]\n",
      "Epoch 14/60: 0it [00:00, ?it/s]\n",
      "Epoch 15/60: 0it [00:00, ?it/s]\n",
      "Epoch 16/60: 0it [00:00, ?it/s]\n",
      "Epoch 17/60: 0it [00:00, ?it/s]\n",
      "Epoch 18/60: 0it [00:00, ?it/s]\n",
      "Epoch 19/60: 0it [00:00, ?it/s]\n",
      "Epoch 20/60: 0it [00:00, ?it/s]\n",
      "Epoch 21/60: 0it [00:00, ?it/s]\n",
      "Epoch 22/60: 0it [00:00, ?it/s]\n",
      "Epoch 23/60: 0it [00:00, ?it/s]\n",
      "Epoch 24/60: 0it [00:00, ?it/s]\n",
      "Epoch 25/60: 0it [00:00, ?it/s]\n",
      "Epoch 26/60: 0it [00:00, ?it/s]\n",
      "Epoch 27/60: 0it [00:00, ?it/s]\n",
      "Epoch 28/60: 0it [00:00, ?it/s]\n",
      "Epoch 29/60: 0it [00:00, ?it/s]\n",
      "Epoch 30/60: 0it [00:00, ?it/s]\n",
      "Epoch 31/60: 0it [00:00, ?it/s]\n",
      "Epoch 32/60: 0it [00:00, ?it/s]\n",
      "Epoch 33/60: 0it [00:00, ?it/s]\n",
      "Epoch 34/60: 0it [00:00, ?it/s]\n",
      "Epoch 35/60: 0it [00:00, ?it/s]\n",
      "Epoch 36/60: 0it [00:00, ?it/s]\n",
      "Epoch 37/60: 0it [00:00, ?it/s]\n",
      "Epoch 38/60: 0it [00:00, ?it/s]\n",
      "Epoch 39/60: 0it [00:00, ?it/s]\n",
      "Epoch 40/60: 0it [00:00, ?it/s]\n",
      "Epoch 41/60: 0it [00:00, ?it/s]\n",
      "Epoch 42/60: 0it [00:00, ?it/s]\n",
      "Epoch 43/60: 0it [00:00, ?it/s]\n",
      "Epoch 44/60: 0it [00:00, ?it/s]\n",
      "Epoch 45/60: 0it [00:00, ?it/s]\n",
      "Epoch 46/60: 0it [00:00, ?it/s]\n",
      "Epoch 47/60: 0it [00:00, ?it/s]\n",
      "Epoch 48/60: 0it [00:00, ?it/s]\n",
      "Epoch 49/60: 0it [00:00, ?it/s]\n",
      "Epoch 50/60: 0it [00:00, ?it/s]\n",
      "Epoch 51/60: 0it [00:00, ?it/s]\n",
      "Epoch 52/60: 0it [00:00, ?it/s]\n",
      "Epoch 53/60: 0it [00:00, ?it/s]\n",
      "Epoch 54/60: 0it [00:00, ?it/s]\n",
      "Epoch 55/60: 0it [00:00, ?it/s]\n",
      "Epoch 56/60: 0it [00:00, ?it/s]\n",
      "Epoch 57/60: 0it [00:00, ?it/s]\n",
      "Epoch 58/60: 0it [00:00, ?it/s]\n",
      "Epoch 59/60: 0it [00:00, ?it/s]\n",
      "Epoch 60/60: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.14 seconds\n",
      "  anomaly detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2801628/3672607049.py\", line 145, in <module>\n",
      "    print(f\"    f1: {result['f1']:.4f}\")\n",
      "                     ~~~~~~^^^^^^\n",
      "KeyError: 'f1'\n",
      "processing: 100%|██████████| 3/3 [00:03<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  finding best threshold...\n",
      "  results saved to: ../../results/models/sr_cnn/33.csv\n",
      "  ID 33 completed:\n",
      "    best_threshold: 0.4850\n",
      "  processing 33 failed: 'f1'\n",
      "summary results saved to: ../../results/models/sr_cnn/sr_cnn.csv\n",
      "\n",
      "================================================================================\n",
      "LSTM anomaly detection results\n",
      "================================================================================\n",
      "processed 3 datasets\n",
      "average F1: 0.2084 ± 0.0715\n",
      "average precision: 0.1430 ± 0.0560\n",
      "average recall: 0.3893 ± 0.0791\n",
      "average accuracy: 0.7163 ± 0.3377\n",
      "average training time: 0.45s\n",
      "average scoring time: 0.57s\n",
      "================================================================================\n",
      "details:\n",
      "   id      f1  precision  recall  accuracy  best_threshold    tp     fp  \\\n",
      "0   4  0.1579     0.1034  0.3333    0.9551          0.6093   3.0   26.0   \n",
      "1  17  0.2590     0.1826  0.4452    0.4775          0.4488  65.0  291.0   \n",
      "2  33     NaN        NaN     NaN       NaN          0.4850   NaN    NaN   \n",
      "\n",
      "      tn    fn  \n",
      "0  677.0   6.0  \n",
      "1  275.0  81.0  \n",
      "2    NaN   NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "selected_ids = [4, 17, 33]\n",
    "\n",
    "results_dir = \"../../results/models/sr_cnn\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def find_best_threshold(scores, true_labels, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        # use percentiles as candidate thresholds\n",
    "        thresholds = [np.percentile(scores, p) for p in range(50, 100, 1)]\n",
    "        # add some extra threshold points\n",
    "        thresholds.extend([np.percentile(scores, p) for p in [99.5, 99.9]])\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_threshold = thresholds[0]\n",
    "    best_metrics = {}\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (scores > threshold).astype(int)\n",
    "\n",
    "        # calculate confusion matrix\n",
    "        TP = ((true_labels == 1) & (pred_labels == 1)).sum()\n",
    "        FP = ((true_labels == 0) & (pred_labels == 1)).sum()\n",
    "        TN = ((true_labels == 0) & (pred_labels == 0)).sum()\n",
    "        FN = ((true_labels == 1) & (pred_labels == 0)).sum()\n",
    "\n",
    "        # calculate metrics\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                \"threshold\": threshold,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"fnr\": fnr,\n",
    "                \"fpr\": fpr,\n",
    "                \"tp\": TP,\n",
    "                \"fp\": FP,\n",
    "                \"tn\": TN,\n",
    "                \"fn\": FN,\n",
    "            }\n",
    "\n",
    "    return best_threshold, best_metrics\n",
    "\n",
    "\n",
    "def process_single_id(dataset_id):\n",
    "    print(f\"processing: {dataset_id}\")\n",
    "\n",
    "    train_data = SRYaHooDataset(_id=dataset_id)\n",
    "    test_data = pl.read_csv(f\"../../datasets/Yahoo/test/A1/{dataset_id}.csv\")\n",
    "    complete_data = pl.read_csv(\n",
    "        f\"../../datasets/Yahoo/data/A1Benchmark/{dataset_id}.csv\"\n",
    "    )\n",
    "\n",
    "    print(\"  training model...\")\n",
    "    model = SRCNN(epochs=60)\n",
    "    start_time = time.time()\n",
    "    model.fit(train_data)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    print(\"  anomaly detection...\")\n",
    "    start_time = time.time()\n",
    "    scores = model.predict(test_data[\"value\"])\n",
    "    scoring_time = time.time() - start_time\n",
    "\n",
    "    train_len = len(\n",
    "        pl.read_csv(f\"../../datasets/Yahoo/train/A1/{dataset_id}.csv\")[\"value\"]\n",
    "    )\n",
    "    test_len = len(complete_data) - train_len\n",
    "    test_true_labels = complete_data[\"label\"][\n",
    "        train_len : train_len + test_len\n",
    "    ].to_numpy()\n",
    "    scores_array = scores.to_numpy()\n",
    "\n",
    "    print(\"  finding best threshold...\")\n",
    "    best_threshold, best_metrics = find_best_threshold(scores_array, test_true_labels)\n",
    "\n",
    "    complete_values = complete_data[\"value\"].to_numpy()\n",
    "    complete_labels = complete_data[\"label\"].to_numpy()\n",
    "    complete_timestamps = (\n",
    "        complete_data[\"timestamp\"]\n",
    "        if \"timestamp\" in complete_data.columns\n",
    "        else range(len(complete_values))\n",
    "    )\n",
    "\n",
    "    complete_predictions = np.zeros(len(complete_values))\n",
    "    complete_predictions[train_len : train_len + test_len] = (\n",
    "        scores_array > best_threshold\n",
    "    ).astype(int)\n",
    "\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"timestamp\": complete_timestamps,\n",
    "            \"value\": complete_values,\n",
    "            \"label\": complete_labels,\n",
    "            \"predicted\": complete_predictions,\n",
    "            \"anomaly_score\": np.concatenate(\n",
    "                [\n",
    "                    np.zeros(train_len),\n",
    "                    scores_array,\n",
    "                    np.zeros(len(complete_values) - train_len - test_len),\n",
    "                ]\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_file = os.path.join(results_dir, f\"{dataset_id}.csv\")\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"  results saved to: {output_file}\")\n",
    "\n",
    "    return {\n",
    "        \"id\": dataset_id,\n",
    "        \"training_time\": training_time,\n",
    "        \"testing_time\": scoring_time,\n",
    "        \"total_time\": training_time + scoring_time,\n",
    "        \"train_samples\": len(train_data),\n",
    "        \"test_samples\": test_len,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        **best_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "all_results = []\n",
    "print(f\"processing {len(selected_ids)} datasets...\")\n",
    "\n",
    "for selected_id in tqdm(selected_ids, desc=\"processing\"):\n",
    "    try:\n",
    "        result = process_single_id(selected_id)\n",
    "        all_results.append(result)\n",
    "\n",
    "        print(f\"  ID {selected_id} completed:\")\n",
    "        print(f\"    best_threshold: {result['best_threshold']:.4f}\")\n",
    "        print(f\"    f1: {result['f1']:.4f}\")\n",
    "        print(f\"    precision: {result['precision']:.4f}\")\n",
    "        print(f\"    recall: {result['recall']:.4f}\")\n",
    "        print(f\"    accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  processing {selected_id} failed: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "if all_results:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    summary_file = os.path.join(results_dir, \"sr_cnn.csv\")\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"summary results saved to: {summary_file}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LSTM anomaly detection results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"processed {len(all_results)} datasets\")\n",
    "    print(f\"average F1: {summary_df['f1'].mean():.4f} ± {summary_df['f1'].std():.4f}\")\n",
    "    print(\n",
    "        f\"average precision: {summary_df['precision'].mean():.4f} ± {summary_df['precision'].std():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"average recall: {summary_df['recall'].mean():.4f} ± {summary_df['recall'].std():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"average accuracy: {summary_df['accuracy'].mean():.4f} ± {summary_df['accuracy'].std():.4f}\"\n",
    "    )\n",
    "    print(f\"average training time: {summary_df['training_time'].mean():.2f}s\")\n",
    "    print(f\"average scoring time: {summary_df['testing_time'].mean():.2f}s\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"details:\")\n",
    "    display_cols = [\n",
    "        \"id\",\n",
    "        \"f1\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"accuracy\",\n",
    "        \"best_threshold\",\n",
    "        \"tp\",\n",
    "        \"fp\",\n",
    "        \"tn\",\n",
    "        \"fn\",\n",
    "    ]\n",
    "    print(summary_df[display_cols].round(4))\n",
    "else:\n",
    "    print(\"no results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6af64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SR变换可视化分析\n",
    "import matplotlib.pyplot as plt\n",
    "from spectrum.models.sr.saliency import Saliency\n",
    "from spectrum.models.sr.utils import marge_series, series_filter\n",
    "\n",
    "\n",
    "def visualize_sr_transformation(dataset_id, model=None):\n",
    "    \"\"\"\n",
    "    可视化SR变换的完整过程\n",
    "\n",
    "    Args:\n",
    "        dataset_id: 数据集ID\n",
    "        model: SpectralResidual模型实例（可选）\n",
    "    \"\"\"\n",
    "\n",
    "    # 加载数据\n",
    "    test_data = pl.read_csv(f\"../../datasets/Yahoo/test/A1/{dataset_id}.csv\")\n",
    "    complete_data = pl.read_csv(\n",
    "        f\"../../datasets/Yahoo/data/A1Benchmark/{dataset_id}.csv\"\n",
    "    )\n",
    "\n",
    "    values = test_data[\"value\"].to_numpy()\n",
    "\n",
    "    # 创建或使用提供的模型\n",
    "    if model is None:\n",
    "        model = SpectralResidual()\n",
    "\n",
    "    # 获取异常分数\n",
    "    anomaly_scores = model.predict(test_data[\"value\"])\n",
    "\n",
    "    # 获取中间变换结果\n",
    "    saliency = Saliency(\n",
    "        amp_window_size=model.mag_window_size,\n",
    "        series_window_size=model.window_size,\n",
    "        score_window_size=model.score_window_size,\n",
    "    )\n",
    "\n",
    "    # 1. 扩展序列\n",
    "    extended_series = marge_series(\n",
    "        values, saliency.series_window_size, saliency.series_window_size\n",
    "    )\n",
    "\n",
    "    # 2. 频谱残差变换\n",
    "    spectral_residual = saliency.transform_spectral_residual(extended_series)\n",
    "    spectral_residual_original = spectral_residual[: len(values)]\n",
    "\n",
    "    # 3. 滤波后的幅度\n",
    "    filtered_mag = series_filter(spectral_residual_original, saliency.score_window_size)\n",
    "\n",
    "    # 创建可视化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(\n",
    "        f\"SR变换过程分析 - 数据集 {dataset_id}\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    time_axis = np.arange(len(values))\n",
    "    extended_time_axis = np.arange(len(extended_series))\n",
    "\n",
    "    # 子图1: 原始数据 vs 扩展数据\n",
    "    axes[0, 0].plot(\n",
    "        time_axis, values, \"b-\", linewidth=2, label=\"Original Data\", alpha=0.8\n",
    "    )\n",
    "    axes[0, 0].plot(\n",
    "        extended_time_axis,\n",
    "        extended_series,\n",
    "        \"r--\",\n",
    "        linewidth=1,\n",
    "        label=\"Extended Data\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    axes[0, 0].axvline(\n",
    "        x=len(values) - 1,\n",
    "        color=\"gray\",\n",
    "        linestyle=\":\",\n",
    "        alpha=0.7,\n",
    "        label=\"Extension Boundary\",\n",
    "    )\n",
    "    axes[0, 0].set_title(\"Original vs Extended Data\")\n",
    "    axes[0, 0].set_xlabel(\"Time Points\")\n",
    "    axes[0, 0].set_ylabel(\"Values\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 子图2: 频谱残差\n",
    "    axes[0, 1].plot(\n",
    "        time_axis,\n",
    "        spectral_residual_original,\n",
    "        \"g-\",\n",
    "        linewidth=2,\n",
    "        label=\"Spectral Residual\",\n",
    "    )\n",
    "    axes[0, 1].plot(\n",
    "        time_axis,\n",
    "        filtered_mag,\n",
    "        \"orange\",\n",
    "        linewidth=2,\n",
    "        label=\"Filtered Magnitude\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    axes[0, 1].set_title(\"Spectral Residual Transform\")\n",
    "    axes[0, 1].set_xlabel(\"Time Points\")\n",
    "    axes[0, 1].set_ylabel(\"Magnitude\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 子图3: 异常分数\n",
    "    axes[1, 0].plot(\n",
    "        time_axis, anomaly_scores, \"purple\", linewidth=2, label=\"Anomaly Scores\"\n",
    "    )\n",
    "    axes[1, 0].fill_between(time_axis, anomaly_scores, alpha=0.3, color=\"purple\")\n",
    "    axes[1, 0].set_title(\"Anomaly Scores\")\n",
    "    axes[1, 0].set_xlabel(\"Time Points\")\n",
    "    axes[1, 0].set_ylabel(\"Anomaly Score\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 子图4: 标准化对比\n",
    "    def standardize(data):\n",
    "        return (data - np.mean(data)) / (np.std(data) + 1e-8)\n",
    "\n",
    "    norm_values = standardize(values)\n",
    "    norm_spectral = standardize(spectral_residual_original)\n",
    "    norm_anomaly = standardize(anomaly_scores)\n",
    "\n",
    "    axes[1, 1].plot(\n",
    "        time_axis, norm_values, \"b-\", linewidth=2, label=\"Original Data\", alpha=0.8\n",
    "    )\n",
    "    axes[1, 1].plot(\n",
    "        time_axis,\n",
    "        norm_spectral,\n",
    "        \"g-\",\n",
    "        linewidth=2,\n",
    "        label=\"Spectral Residual\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    axes[1, 1].plot(\n",
    "        time_axis,\n",
    "        norm_anomaly,\n",
    "        \"purple\",\n",
    "        linewidth=2,\n",
    "        label=\"Anomaly Scores\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    axes[1, 1].set_title(\"Normalized Comparison\")\n",
    "    axes[1, 1].set_xlabel(\"Time Points\")\n",
    "    axes[1, 1].set_ylabel(\"Normalized Values\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 打印统计信息\n",
    "    print(\"=== SR Transform Statistics ===\")\n",
    "    print(f\"Data Length: {len(values)}\")\n",
    "    print(f\"Original Data Range: [{np.min(values):.4f}, {np.max(values):.4f}]\")\n",
    "    print(\n",
    "        f\"Spectral Residual Range: [{np.min(spectral_residual_original):.4f}, {np.max(spectral_residual_original):.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Anomaly Scores Range: [{np.min(anomaly_scores):.4f}, {np.max(anomaly_scores):.4f}]\"\n",
    "    )\n",
    "    print(f\"Mean Anomaly Score: {np.mean(anomaly_scores):.4f}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_anomaly_detection_results(dataset_id, threshold=None):\n",
    "    \"\"\"\n",
    "    可视化异常检测结果\n",
    "    \"\"\"\n",
    "    # 加载数据\n",
    "    test_data = pl.read_csv(f\"../../datasets/Yahoo/test/A1/{dataset_id}.csv\")\n",
    "    complete_data = pl.read_csv(\n",
    "        f\"../../datasets/Yahoo/data/A1Benchmark/{dataset_id}.csv\"\n",
    "    )\n",
    "\n",
    "    values = test_data[\"value\"].to_numpy()\n",
    "\n",
    "    # 获取真实标签\n",
    "    train_len = len(\n",
    "        pl.read_csv(f\"../../datasets/Yahoo/train/A1/{dataset_id}.csv\")[\"value\"]\n",
    "    )\n",
    "    test_len = len(complete_data) - train_len\n",
    "    true_labels = complete_data[\"label\"][train_len : train_len + test_len].to_numpy()\n",
    "\n",
    "    # 获取异常分数\n",
    "    model = SpectralResidual()\n",
    "    anomaly_scores = model.predict(test_data[\"value\"])\n",
    "\n",
    "    # 如果没有提供阈值，使用最佳阈值\n",
    "    if threshold is None:\n",
    "        threshold, _ = find_best_threshold(anomaly_scores, true_labels)\n",
    "\n",
    "    # 识别异常点\n",
    "    predicted_anomalies = anomaly_scores > threshold\n",
    "    true_anomalies = true_labels == 1\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "    fig.suptitle(\n",
    "        f\"Anomaly Detection Results - Dataset {dataset_id}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    time_axis = np.arange(len(values))\n",
    "\n",
    "    # 子图1: 原始数据与真实异常点\n",
    "    axes[0].plot(time_axis, values, \"b-\", linewidth=2, label=\"Original Data\", alpha=0.8)\n",
    "    if np.any(true_anomalies):\n",
    "        axes[0].scatter(\n",
    "            time_axis[true_anomalies],\n",
    "            values[true_anomalies],\n",
    "            color=\"red\",\n",
    "            s=60,\n",
    "            label=f\"True Anomalies ({np.sum(true_anomalies)})\",\n",
    "            zorder=5,\n",
    "        )\n",
    "    axes[0].set_title(\"Original Data with True Anomalies\")\n",
    "    axes[0].set_xlabel(\"Time Points\")\n",
    "    axes[0].set_ylabel(\"Values\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 子图2: 原始数据与预测异常点\n",
    "    axes[1].plot(time_axis, values, \"b-\", linewidth=2, label=\"Original Data\", alpha=0.8)\n",
    "    if np.any(predicted_anomalies):\n",
    "        axes[1].scatter(\n",
    "            time_axis[predicted_anomalies],\n",
    "            values[predicted_anomalies],\n",
    "            color=\"orange\",\n",
    "            s=60,\n",
    "            label=f\"Predicted Anomalies ({np.sum(predicted_anomalies)})\",\n",
    "            zorder=5,\n",
    "        )\n",
    "    axes[1].set_title(\n",
    "        f\"Original Data with Predicted Anomalies (Threshold: {threshold:.4f})\"\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Time Points\")\n",
    "    axes[1].set_ylabel(\"Values\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 子图3: 异常分数与阈值\n",
    "    axes[2].plot(\n",
    "        time_axis, anomaly_scores, \"purple\", linewidth=2, label=\"Anomaly Scores\"\n",
    "    )\n",
    "    axes[2].fill_between(time_axis, anomaly_scores, alpha=0.3, color=\"purple\")\n",
    "    axes[2].axhline(\n",
    "        y=threshold,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Threshold ({threshold:.4f})\",\n",
    "    )\n",
    "\n",
    "    # 高亮异常区域\n",
    "    if np.any(predicted_anomalies):\n",
    "        axes[2].fill_between(\n",
    "            time_axis,\n",
    "            threshold,\n",
    "            anomaly_scores,\n",
    "            where=(anomaly_scores > threshold),\n",
    "            color=\"red\",\n",
    "            alpha=0.3,\n",
    "            label=\"Anomaly Regions\",\n",
    "        )\n",
    "\n",
    "    axes[2].set_title(\"Anomaly Scores and Threshold\")\n",
    "    axes[2].set_xlabel(\"Time Points\")\n",
    "    axes[2].set_ylabel(\"Anomaly Score\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 计算性能指标\n",
    "    TP = np.sum((true_anomalies == 1) & (predicted_anomalies == 1))\n",
    "    FP = np.sum((true_anomalies == 0) & (predicted_anomalies == 1))\n",
    "    TN = np.sum((true_anomalies == 0) & (predicted_anomalies == 0))\n",
    "    FN = np.sum((true_anomalies == 1) & (predicted_anomalies == 0))\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = (\n",
    "        2 * (precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "\n",
    "    print(\"=== Detection Results ===\")\n",
    "    print(f\"True Anomalies: {np.sum(true_anomalies)}\")\n",
    "    print(f\"Predicted Anomalies: {np.sum(predicted_anomalies)}\")\n",
    "    print(f\"True Positives: {TP}\")\n",
    "    print(f\"False Positives: {FP}\")\n",
    "    print(f\"True Negatives: {TN}\")\n",
    "    print(f\"False Negatives: {FN}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compare_multiple_datasets(dataset_ids=None):\n",
    "    \"\"\"\n",
    "    比较多个数据集的SR变换结果\n",
    "    \"\"\"\n",
    "    if dataset_ids is None:\n",
    "        dataset_ids = selected_ids\n",
    "\n",
    "    fig, axes = plt.subplots(len(dataset_ids), 2, figsize=(16, 6 * len(dataset_ids)))\n",
    "    if len(dataset_ids) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"SR Transform Comparison Across Datasets\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    model = SpectralResidual()\n",
    "\n",
    "    for i, dataset_id in enumerate(dataset_ids):\n",
    "        # 加载数据\n",
    "        test_data = pl.read_csv(f\"../../datasets/Yahoo/test/A1/{dataset_id}.csv\")\n",
    "        values = test_data[\"value\"].to_numpy()\n",
    "\n",
    "        # 获取异常分数\n",
    "        anomaly_scores = model.predict(test_data[\"value\"])\n",
    "\n",
    "        time_axis = np.arange(len(values))\n",
    "\n",
    "        # 原始数据\n",
    "        axes[i, 0].plot(\n",
    "            time_axis, values, \"b-\", linewidth=2, label=f\"Dataset {dataset_id}\"\n",
    "        )\n",
    "        axes[i, 0].set_title(f\"Original Data - Dataset {dataset_id}\")\n",
    "        axes[i, 0].set_xlabel(\"Time Points\")\n",
    "        axes[i, 0].set_ylabel(\"Values\")\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        axes[i, 0].legend()\n",
    "\n",
    "        # 异常分数\n",
    "        axes[i, 1].plot(\n",
    "            time_axis,\n",
    "            anomaly_scores,\n",
    "            \"purple\",\n",
    "            linewidth=2,\n",
    "            label=f\"Dataset {dataset_id}\",\n",
    "        )\n",
    "        axes[i, 1].fill_between(time_axis, anomaly_scores, alpha=0.3, color=\"purple\")\n",
    "        axes[i, 1].set_title(f\"Anomaly Scores - Dataset {dataset_id}\")\n",
    "        axes[i, 1].set_xlabel(\"Time Points\")\n",
    "        axes[i, 1].set_ylabel(\"Anomaly Score\")\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        axes[i, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb57f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SR变换可视化演示 ===\n",
      "\n",
      "1. 分析数据集 4 的SR变换过程:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SpectralResidual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m demo_dataset_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. 分析数据集 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdemo_dataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 的SR变换过程:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mvisualize_sr_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemo_dataset_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. 分析数据集 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdemo_dataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 的异常检测结果:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m visualize_anomaly_detection_results(demo_dataset_id)\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mvisualize_sr_transformation\u001b[0;34m(dataset_id, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 创建或使用提供的模型\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSpectralResidual\u001b[49m()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 获取异常分数\u001b[39;00m\n\u001b[1;32m     29\u001b[0m anomaly_scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SpectralResidual' is not defined"
     ]
    }
   ],
   "source": [
    "# 演示SR变换可视化\n",
    "print(\"=== SR变换可视化演示 ===\")\n",
    "\n",
    "# 选择一个数据集进行详细分析\n",
    "demo_dataset_id = 4\n",
    "\n",
    "print(f\"\\n1. 分析数据集 {demo_dataset_id} 的SR变换过程:\")\n",
    "visualize_sr_transformation(demo_dataset_id)\n",
    "\n",
    "print(f\"\\n2. 分析数据集 {demo_dataset_id} 的异常检测结果:\")\n",
    "visualize_anomaly_detection_results(demo_dataset_id)\n",
    "\n",
    "print(f\"\\n3. 比较所有数据集的结果:\")\n",
    "compare_multiple_datasets(selected_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
